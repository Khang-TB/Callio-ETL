{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b06d42-8356-4865-84ba-b86f9c9a2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 báº£ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60daf6b0-c47b-4b02-8eb8-26c22f41896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 14:30:42+0700 | INFO     | ðŸ”¥ Warmed checkpoint cache: 20 keys (with last_run)\n",
      "2025-09-10 14:30:42+0700 | INFO     | â–¶ Run customer (all tenants) | interval=15m\n",
      "2025-09-10 14:30:43+0700 | INFO     | [hot1new] token obtained\n",
      "2025-09-10 14:30:43+0700 | INFO     | [hot1new][customer] ck=1757409864573 (2025-09-09T09:24:24.573000+00:00) overlap=180000 â†’ cutoff=2025-09-09T09:21:24.573000+00:00\n",
      "2025-09-10 14:30:44+0700 | INFO     | [hot1new][customer] page=1 got=500 cum=500 last_ts=2025-09-10T07:16:45.637000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ1.1% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:45+0700 | INFO     | [hot1new][customer] page=2 got=500 cum=1000 last_ts=2025-09-10T06:41:55.140000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ3.7% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:46+0700 | INFO     | [hot1new][customer] page=3 got=500 cum=1500 last_ts=2025-09-10T04:40:19.814000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ12.8% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:47+0700 | INFO     | [hot1new][customer] page=4 got=500 cum=2000 last_ts=2025-09-10T04:10:09.772000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ15.1% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:48+0700 | INFO     | [hot1new][customer] page=5 got=500 cum=2500 last_ts=2025-09-10T03:37:16.190000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ17.6% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:48+0700 | INFO     | [hot1new][customer] page=6 got=500 cum=3000 last_ts=2025-09-10T02:56:05.706000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ20.7% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:50+0700 | INFO     | [hot1new][customer] page=7 got=500 cum=3500 last_ts=2025-09-10T02:35:18.904000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ22.2% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:50+0700 | INFO     | [hot1new][customer] page=8 got=500 cum=4000 last_ts=2025-09-10T02:06:48.698000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ24.4% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:51+0700 | INFO     | [hot1new][customer] page=9 got=500 cum=4500 last_ts=2025-09-10T01:40:44.454000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ26.3% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:52+0700 | INFO     | [hot1new][customer] page=10 got=500 cum=5000 last_ts=2025-09-09T09:43:33.903000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ98.3% totalDocs=441606 hasNext=True\n",
      "2025-09-10 14:30:53+0700 | INFO     | [hot1new][customer] page=11 got=351 cum=5351 last_ts=2025-09-09T09:21:26.686000+00:00 window=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.265000+00:00] time_coverageâ‰ˆ100.0% totalDocs=441606 hasNext=False\n",
      "2025-09-10 14:30:53+0700 | INFO     | [hot1new][customer] DONE pages=11 loaded=5351 range=[2025-09-09T09:21:24.573000+00:00 â†’ 2025-09-10T07:30:43.161000+00:00]\n",
      "2025-09-10 14:30:57+0700 | INFO     | [hot1new][customer] STAGED rows=5351 window=[2025-09-09..2025-09-10] (ck unchanged)\n",
      "2025-09-10 14:30:57+0700 | INFO     | [hot1old] token obtained\n",
      "2025-09-10 14:30:57+0700 | INFO     | [hot1old][customer] ck=1757409869126 (2025-09-09T09:24:29.126000+00:00) overlap=180000 â†’ cutoff=2025-09-09T09:21:29.126000+00:00\n",
      "2025-09-10 14:30:58+0700 | INFO     | [hot1old][customer] page=1 got=500 cum=500 last_ts=2025-09-10T07:17:00.048000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ1.1% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:30:59+0700 | INFO     | [hot1old][customer] page=2 got=500 cum=1000 last_ts=2025-09-10T04:41:04.324000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ12.8% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:30:59+0700 | INFO     | [hot1old][customer] page=3 got=500 cum=1500 last_ts=2025-09-10T04:06:05.877000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ15.4% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:00+0700 | INFO     | [hot1old][customer] page=4 got=500 cum=2000 last_ts=2025-09-10T03:24:18.213000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ18.6% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:01+0700 | INFO     | [hot1old][customer] page=5 got=500 cum=2500 last_ts=2025-09-10T02:48:39.109000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ21.2% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:02+0700 | INFO     | [hot1old][customer] page=6 got=500 cum=3000 last_ts=2025-09-10T02:14:15.477000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ23.8% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:02+0700 | INFO     | [hot1old][customer] page=7 got=500 cum=3500 last_ts=2025-09-10T01:44:58.474000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ26.0% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:03+0700 | INFO     | [hot1old][customer] page=8 got=500 cum=4000 last_ts=2025-09-09T21:13:45.993000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ46.4% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:04+0700 | INFO     | [hot1old][customer] page=9 got=500 cum=4500 last_ts=2025-09-09T10:16:04.898000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ95.9% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:04+0700 | INFO     | [hot1old][customer] page=10 got=500 cum=5000 last_ts=2025-09-09T09:28:22.941000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ99.5% totalDocs=161710 hasNext=True\n",
      "2025-09-10 14:31:05+0700 | INFO     | [hot1old][customer] page=11 got=98 cum=5098 last_ts=2025-09-09T09:21:29.575000+00:00 window=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.729000+00:00] time_coverageâ‰ˆ100.0% totalDocs=161710 hasNext=False\n",
      "2025-09-10 14:31:05+0700 | INFO     | [hot1old][customer] DONE pages=11 loaded=5098 range=[2025-09-09T09:21:29.126000+00:00 â†’ 2025-09-10T07:30:57.902000+00:00]\n",
      "2025-09-10 14:31:09+0700 | INFO     | [hot1old][customer] STAGED rows=5098 window=[2025-09-09..2025-09-10] (ck unchanged)\n",
      "2025-09-10 14:31:10+0700 | INFO     | [hot2] token obtained\n",
      "2025-09-10 14:31:10+0700 | INFO     | [hot2][customer] ck=1757409874860 (2025-09-09T09:24:34.860000+00:00) overlap=180000 â†’ cutoff=2025-09-09T09:21:34.860000+00:00\n",
      "2025-09-10 14:31:10+0700 | INFO     | [hot2][customer] page=1 got=500 cum=500 last_ts=2025-09-10T07:13:19.749000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ1.3% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:11+0700 | INFO     | [hot2][customer] page=2 got=500 cum=1000 last_ts=2025-09-10T06:29:52.116000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ4.6% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:12+0700 | INFO     | [hot2][customer] page=3 got=500 cum=1500 last_ts=2025-09-10T04:35:08.471000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ13.2% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:12+0700 | INFO     | [hot2][customer] page=4 got=500 cum=2000 last_ts=2025-09-10T04:06:55.861000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ15.4% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:13+0700 | INFO     | [hot2][customer] page=5 got=500 cum=2500 last_ts=2025-09-10T03:45:10.285000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ17.0% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:14+0700 | INFO     | [hot2][customer] page=6 got=500 cum=3000 last_ts=2025-09-10T03:06:29.293000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ19.9% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:15+0700 | INFO     | [hot2][customer] page=7 got=500 cum=3500 last_ts=2025-09-10T02:47:05.871000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ21.4% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:15+0700 | INFO     | [hot2][customer] page=8 got=500 cum=4000 last_ts=2025-09-10T02:04:24.574000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ24.6% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:16+0700 | INFO     | [hot2][customer] page=9 got=500 cum=4500 last_ts=2025-09-10T01:27:28.242000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ27.4% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:16+0700 | INFO     | [hot2][customer] page=10 got=500 cum=5000 last_ts=2025-09-09T21:15:25.671000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ46.3% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:17+0700 | INFO     | [hot2][customer] page=11 got=500 cum=5500 last_ts=2025-09-09T11:47:19.598000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ89.0% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:18+0700 | INFO     | [hot2][customer] page=12 got=500 cum=6000 last_ts=2025-09-09T09:55:07.694000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ97.5% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:19+0700 | INFO     | [hot2][customer] page=13 got=500 cum=6500 last_ts=2025-09-09T09:26:21.642000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ99.6% totalDocs=85658 hasNext=True\n",
      "2025-09-10 14:31:19+0700 | INFO     | [hot2][customer] page=14 got=91 cum=6591 last_ts=2025-09-09T09:21:36.693000+00:00 window=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.119000+00:00] time_coverageâ‰ˆ100.0% totalDocs=85658 hasNext=False\n",
      "2025-09-10 14:31:19+0700 | INFO     | [hot2][customer] DONE pages=14 loaded=6591 range=[2025-09-09T09:21:34.860000+00:00 â†’ 2025-09-10T07:31:10.520000+00:00]\n",
      "2025-09-10 14:31:24+0700 | INFO     | [hot2][customer] STAGED rows=6591 window=[2025-09-09..2025-09-10] (ck unchanged)\n",
      "2025-09-10 14:31:33+0700 | INFO     | ðŸ§© MERGE customer done for window [2025-09-09..2025-09-10] & cleaned staging.\n",
      "2025-09-10 14:31:33+0700 | INFO     | [hot1new][customer] MERGED window [2025-09-09..2025-09-10] â†’ set CK = 2025-09-10T07:30:43.161000+00:00\n",
      "2025-09-10 14:31:33+0700 | INFO     | [hot1old][customer] MERGED window [2025-09-09..2025-09-10] â†’ set CK = 2025-09-10T07:30:57.902000+00:00\n",
      "2025-09-10 14:31:33+0700 | INFO     | [hot2][customer] MERGED window [2025-09-09..2025-09-10] â†’ set CK = 2025-09-10T07:31:10.520000+00:00\n",
      "2025-09-10 14:31:33+0700 | INFO     | â–¶ Run call_log (all tenants) | interval=15m\n",
      "2025-09-10 14:31:33+0700 | INFO     | [hot1new][call_log] ck=1757409886969 (2025-09-09T09:24:46.969000+00:00) â†’ cutoff=1757409886969 (2025-09-09T09:24:46.969000+00:00); window_end=now=2025-09-10T07:31:33.198000+00:00\n",
      "2025-09-10 14:31:34+0700 | INFO     | [hot1new][call_log] page=1 got=500 cum=500 last_ts=2025-09-10T07:13:54.885000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ1.3% totalDocs=197212 hasNext=True\n",
      "2025-09-10 14:31:34+0700 | INFO     | [hot1new][call_log] page=2 got=500 cum=1000 last_ts=2025-09-10T06:49:44.942000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ3.2% totalDocs=197213 hasNext=True\n",
      "2025-09-10 14:31:35+0700 | INFO     | [hot1new][call_log] page=3 got=500 cum=1500 last_ts=2025-09-10T04:52:08.534000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ12.0% totalDocs=197213 hasNext=True\n",
      "2025-09-10 14:31:36+0700 | INFO     | [hot1new][call_log] page=4 got=500 cum=2000 last_ts=2025-09-10T04:28:40.684000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ13.8% totalDocs=197213 hasNext=True\n",
      "2025-09-10 14:31:37+0700 | INFO     | [hot1new][call_log] page=5 got=500 cum=2500 last_ts=2025-09-10T04:05:17.375000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ15.5% totalDocs=197213 hasNext=True\n",
      "2025-09-10 14:31:37+0700 | INFO     | [hot1new][call_log] page=6 got=500 cum=3000 last_ts=2025-09-10T03:42:59.244000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ17.2% totalDocs=197215 hasNext=True\n",
      "2025-09-10 14:31:38+0700 | INFO     | [hot1new][call_log] page=7 got=500 cum=3500 last_ts=2025-09-10T03:15:29.030000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ19.3% totalDocs=197216 hasNext=True\n",
      "2025-09-10 14:31:39+0700 | INFO     | [hot1new][call_log] page=8 got=500 cum=4000 last_ts=2025-09-10T02:55:54.806000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ20.8% totalDocs=197216 hasNext=True\n",
      "2025-09-10 14:31:40+0700 | INFO     | [hot1new][call_log] page=9 got=500 cum=4500 last_ts=2025-09-10T02:37:30.909000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ22.2% totalDocs=197216 hasNext=True\n",
      "2025-09-10 14:31:40+0700 | INFO     | [hot1new][call_log] page=10 got=500 cum=5000 last_ts=2025-09-10T02:21:25.010000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ23.4% totalDocs=197216 hasNext=True\n",
      "2025-09-10 14:31:42+0700 | INFO     | [hot1new][call_log] page=11 got=500 cum=5500 last_ts=2025-09-10T01:47:23.453000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ25.9% totalDocs=197217 hasNext=True\n",
      "2025-09-10 14:31:43+0700 | INFO     | [hot1new][call_log] page=12 got=500 cum=6000 last_ts=2025-09-09T10:01:17.064000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ97.2% totalDocs=197217 hasNext=True\n",
      "2025-09-10 14:31:44+0700 | INFO     | [hot1new][call_log] page=13 got=500 cum=6500 last_ts=2025-09-09T09:26:41.250000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ99.9% totalDocs=197217 hasNext=True\n",
      "2025-09-10 14:31:44+0700 | INFO     | [hot1new][call_log] page=14 got=33 cum=6533 last_ts=2025-09-09T09:25:18.146000+00:00 window=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:33.199000+00:00] time_coverageâ‰ˆ100.0% totalDocs=197217 hasNext=False\n",
      "2025-09-10 14:31:44+0700 | INFO     | [hot1new][call_log] DONE pages=14 loaded=6533 range=[2025-09-09T09:24:46.969000+00:00 â†’ 2025-09-10T07:31:30.179000+00:00]\n",
      "2025-09-10 14:31:48+0700 | INFO     | [hot1new][call_log] APPEND loaded=6528 new_ck=1757489490179 (2025-09-10T07:31:30.179000+00:00)\n",
      "2025-09-10 14:31:48+0700 | INFO     | [hot1old][call_log] ck=1757409890578 (2025-09-09T09:24:50.578000+00:00) â†’ cutoff=1757409890578 (2025-09-09T09:24:50.578000+00:00); window_end=now=2025-09-10T07:31:48.499000+00:00\n",
      "2025-09-10 14:31:48+0700 | WARNING  | [hot1old][call_log] 401 on page=1 â†’ refreshing token and retry once\n",
      "2025-09-10 14:31:49+0700 | INFO     | [hot1old] token refreshed\n",
      "2025-09-10 14:31:50+0700 | INFO     | [hot1old][call_log] page=1 got=500 cum=500 last_ts=2025-09-10T06:58:28.082000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ2.5% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:51+0700 | INFO     | [hot1old][call_log] page=2 got=500 cum=1000 last_ts=2025-09-10T04:32:55.012000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ13.5% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:51+0700 | INFO     | [hot1old][call_log] page=3 got=500 cum=1500 last_ts=2025-09-10T04:05:54.755000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ15.5% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:52+0700 | INFO     | [hot1old][call_log] page=4 got=500 cum=2000 last_ts=2025-09-10T03:30:56.887000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ18.2% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:53+0700 | INFO     | [hot1old][call_log] page=5 got=500 cum=2500 last_ts=2025-09-10T03:02:23.163000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ20.3% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:54+0700 | INFO     | [hot1old][call_log] page=6 got=500 cum=3000 last_ts=2025-09-10T02:35:11.293000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ22.4% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:55+0700 | INFO     | [hot1old][call_log] page=7 got=500 cum=3500 last_ts=2025-09-10T02:09:12.998000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ24.3% totalDocs=137833 hasNext=True\n",
      "2025-09-10 14:31:55+0700 | INFO     | [hot1old][call_log] page=8 got=500 cum=4000 last_ts=2025-09-10T01:42:14.101000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ26.3% totalDocs=137834 hasNext=True\n",
      "2025-09-10 14:31:56+0700 | INFO     | [hot1old][call_log] page=9 got=500 cum=4500 last_ts=2025-09-09T10:18:06.578000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ96.0% totalDocs=137834 hasNext=True\n",
      "2025-09-10 14:31:57+0700 | INFO     | [hot1old][call_log] page=10 got=500 cum=5000 last_ts=2025-09-09T09:39:20.646000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ98.9% totalDocs=137834 hasNext=True\n",
      "2025-09-10 14:31:58+0700 | INFO     | [hot1old][call_log] page=11 got=192 cum=5192 last_ts=2025-09-09T09:25:11.844000+00:00 window=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.500000+00:00] time_coverageâ‰ˆ100.0% totalDocs=137834 hasNext=False\n",
      "2025-09-10 14:31:58+0700 | INFO     | [hot1old][call_log] DONE pages=11 loaded=5192 range=[2025-09-09T09:24:50.578000+00:00 â†’ 2025-09-10T07:31:48.907000+00:00]\n",
      "2025-09-10 14:32:01+0700 | INFO     | [hot1old][call_log] APPEND loaded=5191 new_ck=1757489508925 (2025-09-10T07:31:48.925000+00:00)\n",
      "2025-09-10 14:32:01+0700 | INFO     | [hot2][call_log] ck=1757409892405 (2025-09-09T09:24:52.405000+00:00) â†’ cutoff=1757409892405 (2025-09-09T09:24:52.405000+00:00); window_end=now=2025-09-10T07:32:01.574000+00:00\n",
      "2025-09-10 14:32:02+0700 | INFO     | [hot2][call_log] page=1 got=500 cum=500 last_ts=2025-09-10T07:09:28.382000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ1.7% totalDocs=150947 hasNext=True\n",
      "2025-09-10 14:32:03+0700 | INFO     | [hot2][call_log] page=2 got=500 cum=1000 last_ts=2025-09-10T06:47:27.075000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ3.4% totalDocs=150947 hasNext=True\n",
      "2025-09-10 14:32:03+0700 | INFO     | [hot2][call_log] page=3 got=500 cum=1500 last_ts=2025-09-10T04:45:28.256000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ12.5% totalDocs=150948 hasNext=True\n",
      "2025-09-10 14:32:04+0700 | INFO     | [hot2][call_log] page=4 got=500 cum=2000 last_ts=2025-09-10T04:22:28.727000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ14.3% totalDocs=150948 hasNext=True\n",
      "2025-09-10 14:32:05+0700 | INFO     | [hot2][call_log] page=5 got=500 cum=2500 last_ts=2025-09-10T04:00:36.308000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ15.9% totalDocs=150948 hasNext=True\n",
      "2025-09-10 14:32:06+0700 | INFO     | [hot2][call_log] page=6 got=500 cum=3000 last_ts=2025-09-10T03:39:01.504000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ17.6% totalDocs=150949 hasNext=True\n",
      "2025-09-10 14:32:06+0700 | INFO     | [hot2][call_log] page=7 got=500 cum=3500 last_ts=2025-09-10T03:08:56.908000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ19.8% totalDocs=150949 hasNext=True\n",
      "2025-09-10 14:32:07+0700 | INFO     | [hot2][call_log] page=8 got=500 cum=4000 last_ts=2025-09-10T02:33:43.177000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ22.5% totalDocs=150951 hasNext=True\n",
      "2025-09-10 14:32:08+0700 | INFO     | [hot2][call_log] page=9 got=500 cum=4500 last_ts=2025-09-10T02:05:05.055000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ24.6% totalDocs=150952 hasNext=True\n",
      "2025-09-10 14:32:08+0700 | INFO     | [hot2][call_log] page=10 got=500 cum=5000 last_ts=2025-09-10T01:41:33.213000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ26.4% totalDocs=150952 hasNext=True\n",
      "2025-09-10 14:32:09+0700 | INFO     | [hot2][call_log] page=11 got=500 cum=5500 last_ts=2025-09-09T10:51:22.865000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ93.5% totalDocs=150952 hasNext=True\n",
      "2025-09-10 14:32:10+0700 | INFO     | [hot2][call_log] page=12 got=500 cum=6000 last_ts=2025-09-09T09:56:33.445000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ97.6% totalDocs=150953 hasNext=True\n",
      "2025-09-10 14:32:11+0700 | INFO     | [hot2][call_log] page=13 got=500 cum=6500 last_ts=2025-09-09T09:32:07.799000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ99.5% totalDocs=150953 hasNext=True\n",
      "2025-09-10 14:32:11+0700 | INFO     | [hot2][call_log] page=14 got=151 cum=6651 last_ts=2025-09-09T09:25:08.012000+00:00 window=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:32:01.575000+00:00] time_coverageâ‰ˆ100.0% totalDocs=150953 hasNext=False\n",
      "2025-09-10 14:32:11+0700 | INFO     | [hot2][call_log] DONE pages=14 loaded=6651 range=[2025-09-09T09:24:52.405000+00:00 â†’ 2025-09-10T07:31:31.246000+00:00]\n",
      "2025-09-10 14:32:16+0700 | INFO     | [hot2][call_log] APPEND loaded=6645 new_ck=1757489520948 (2025-09-10T07:32:00.948000+00:00)\n",
      "2025-09-10 14:32:16+0700 | INFO     | â–¶ Daily rank_mapping â†’ writing weekly table key=2025w37\n",
      "2025-09-10 14:32:24+0700 | INFO     | [rank_mapping] APPEND rows=149 for week=2025w37 (week_start=2025-09-08)\n",
      "2025-09-10 14:32:24+0700 | INFO     | [rank_mapping] REPLACE_PARTITION week=2025w37 rows=149 â†’ table=rank_mapping\n",
      "2025-09-10 14:32:26+0700 | INFO     | ðŸ“ Flushed update_log: 10 rows\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  ETL Callio -> BigQuery (Notebook Runner)\n",
    "#  - Warm checkpoint cache (update_log) 1 láº§n\n",
    "#  - Loop vÃ´ háº¡n, tá»± lÃªn lá»‹ch theo káº¿ hoáº¡ch:\n",
    "#       + customer: 15 phÃºt (delta theo updateTime, DESC + early-stop, overlap)\n",
    "#       + call_log: 15 phÃºt (delta theo createTime, DESC + early-stop, overlap)\n",
    "#       + staff/group: 1 láº§n/ngÃ y (snapshot WRITE_TRUNCATE)\n",
    "#       + rank_mapping: 1 láº§n/tuáº§n (snapshot WRITE_TRUNCATE)  â†’ placeholder\n",
    "#  - Buffer update_log, flush 1 phÃ¡t má»—i vÃ²ng\n",
    "#  - Giá»¯ schema y nhÆ° áº£nh (cÃ³ bÆ°á»›c táº¡o báº£ng náº¿u thiáº¿u)\n",
    "# =========================================\n",
    "\n",
    "import os, sys, json, time, logging, hashlib, math\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound, Conflict, BadRequest\n",
    "\n",
    "# ---------- 1) CREDENTIALS ----------\n",
    "# DÃN Service Account JSON vÃ o Ä‘Ã¢y (hoáº·c set env SERVICE_ACCOUNT_KEY_JSON)\n",
    "SERVICE_ACCOUNT_KEY_JSON = os.getenv(\"SERVICE_ACCOUNT_KEY_JSON\") or r\"\"\"{\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"rio-system-migration\",\n",
    "    \"private_key_id\": \"30a7b13765330a0a061b667aa13f03b72cfe344e\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCmrqE5UEBPBbSv\\ndiOy+nnckHlLHww8Icx+Fma+GxWWaMnhAviX4v/7ED3b9UTwfB8qWYIf0EmETuGA\\nMNXfD1Zk5afZLNg+KfdXCxALZ71+Rd1oxa9fpOTJWD30SvK2GwZk2T/1KvAlsefm\\nlN9EBX2PQ+YteZUh5n2HTcFTZjlrrHVfoRGWDzQdLOX+ZG8JSROybuhZ1ZC0Ebrg\\nCcsry2o+RrQYigaNYgs8EdP/O1iD/jCX+DYigCjIxNqVGg0SIRUJoXJOao6LzUhP\\n0BNuji7GzkFQ2vzXFc50FNrmE5lqTCSAfSSuyItSWux/KMhgTP+hJVh5rACGmUKL\\njPy7q4A7AgMBAAECggEAAR7OXZzA+eb/amiAX+0YEZf1AMDCK8tMXRKYeTGkaQDm\\nTnlfI2I2t0DKMabJ4lXrPbUhT0ZuoBGJqXtqjJfFTGNQyABpqa1NiMu9vV3io60j\\nilc90QZCNWo+7FvumrO+zMG7ENJLj+1MsjuQ7puc1vAtZWPaUUKD2Ht4z68xtpwM\\n2nPXsNhhJKq2rXMKfERk6cDl7dILskJWaHuTs4rYu65eVtfBRDgTlULLQ0ONxm/W\\n6apECFuM8K1R3TwX5XDUEGokyDsAz77ukRAanAEXK8XtADmX7VFK5X7hUtG1+b8p\\nntVAbx2dAZ5W+PHjv81LEXRbqEcC1JJcD4PkH0plNQKBgQDg6qnIvpb2iR79+6Kh\\nU0Do7POsqCgtN0Etex/EDjM71xo21gdZi7g2BJ0RAsblFxqdPsRaHDZZXJcs2R8G\\nqlm+6zs04c8oz/lU6PxTaBrSHBixMdhUvk/YGrudN7YAN4/RLt/loZzP3/BAgco9\\nlLaPckHJNFVETuKO3CgfXaJPnQKBgQC9t69totWF59MCW9Z4+TnAVW14TEetHhD2\\nsC7Ju+6dGPLxXlnuZuBJQIkWo6T0cqJIYF0BumYdKltGjA9KFyhjDbcydJPRZM2q\\nwo0KD1entyXblYMBEMzP0zsgN3+3Jwg+ZlHUxtK+gjKJiTfISWI1ogBDD3vd4G36\\n8yB/sgDDtwKBgDqKmbqYcO6mbhypfIEFDGYUFrCf7CUotpxB6di74XX33Ojc+HjE\\nNyRIOyGMWXyTcOfwyGaz5SmJQgf4U20GtelNjNGM3MDAsSL6qYKEHEcH7R1h3e7g\\nwiN7gc3ADG0uCQ7nZnt8fzZUEVKY9azlokbf9GOMbY0kAzAv+XmAg5i5AoGAT525\\nWio+r05FaDUAQY5dpRB0u0pPvh/jAJOZXwGmNnlU4uQ0m27C6xrRLRYJ0KgW4IbI\\nIUSHO/Adk/KNLAuh4EfOPLddnT9PbDzvEWy03WZn1cndy2GwgfrkUjXYPBV+SSmJ\\nZ+D0agybhsp2BXB+bYGJ2Jqz4b4giXLkjZI27esCgYAQ3iMcyjM3J8UcTb+AMYAW\\nnAFFRcRvvmVtBSlSzCt+K3d9TZpS1fx9+cBi7o842nIGXvg3lppR4N5F7YrrR25I\\ne5zFrYWTfm+beMyvUYmdVjhoO6PclZWXZMykqVMFichTSS8yLXP08y5s7XJUGc18\\n0Kpii9feusS8ogWahhwwQw==\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"rio-bigquery@rio-system-migration.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"116167675015485282559\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/rio-bigquery%40rio-system-migration.iam.gserviceaccount.com\"\n",
    "}\"\"\"\n",
    "\n",
    "# DÃN danh sÃ¡ch tenant Callio (hoáº·c set env CALLIO_ACCOUNTS_JSON)\n",
    "# Má»—i pháº§n tá»­: {\"tenant\":\"hot1new\",\"email\":\"...\",\"password\":\"...\"}\n",
    "ACCOUNTS = json.loads(os.getenv(\"CALLIO_ACCOUNTS_JSON\", \"[]\")) or [\n",
    "    {\"tenant\":\"hot1new\",\"email\":\"hot1new@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "    {\"tenant\":\"hot1old\",\"email\":\"hot1old@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "    {\"tenant\":\"hot2\",   \"email\":\"hot2@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "]\n",
    "# ---------- Rank Mapping Config ----------\n",
    "import re, unicodedata, gspread\n",
    "from google.oauth2 import service_account as gsa\n",
    "\n",
    "RANK_SHEET_ID   = os.getenv(\"RANK_SHEET_ID\", \"1U8B9tglIj21GRbC-TJfB549hx7xE-kP6X99ls-Ihsho\")\n",
    "RANK_TAB_NAME   = os.getenv(\"RANK_TAB_NAME\", \"Xáº¾P Háº NG HIá»†N Táº I\")\n",
    "RANK_RANGES_A1 = os.getenv(\"RANK_RANGES_A1\", \"A1:Q\").split(\";\")\n",
    "# ---------- 2) CONFIG ----------\n",
    "CALLIO_API_BASE_URL = os.getenv(\"CALLIO_API_BASE_URL\", \"https://clientapi.phonenet.io\")\n",
    "API_TIMEOUT   = int(os.getenv(\"API_TIMEOUT\", \"90\"))\n",
    "API_PAGE_SIZE = int(os.getenv(\"API_PAGE_SIZE\", \"500\"))\n",
    "\n",
    "BQ_PROJECT_ID = os.getenv(\"BQ_PROJECT_ID\", \"rio-system-migration\")\n",
    "BQ_DATASET_ID = os.getenv(\"BQ_DATASET_ID\", \"dev_callio\")\n",
    "BQ_LOCATION   = os.getenv(\"BQ_LOCATION\", \"asia-southeast1\")\n",
    "\n",
    "# Lá»‹ch cháº¡y\n",
    "INTERVAL_CUSTOMER_MIN = 15\n",
    "INTERVAL_CALL_MIN     = 15\n",
    "RUN_DAILY_HOUR        = 9  # staff/group má»—i ngÃ y\n",
    "RUN_RANK_DAILY_HOUR   = 9  # rank_mapping má»—i ngÃ y\n",
    "\n",
    "# Cá»­a sá»• vÃ  checkpoint\n",
    "OVERLAP_MS = int(os.getenv(\"OVERLAP_MS\", str(3 * 60 * 1000)))      # 3 phÃºt an toÃ n\n",
    "DAYS_TO_FETCH_IF_EMPTY = int(os.getenv(\"DAYS_TO_FETCH_IF_EMPTY\", \"30\"))\n",
    "\n",
    "# Giá»›i háº¡n Ä‘á»ƒ test (None = khÃ´ng giá»›i háº¡n)\n",
    "LIMIT_RECORDS_PER_ENDPOINT: Optional[int] = None\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S%z\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(\"runner\")\n",
    "\n",
    "# ---------- 3) BQ CLIENT ----------\n",
    "def get_bq_client() -> bigquery.Client:\n",
    "    info = json.loads(SERVICE_ACCOUNT_KEY_JSON)\n",
    "    project = info.get(\"project_id\") or BQ_PROJECT_ID\n",
    "    return bigquery.Client.from_service_account_info(info, project=project)\n",
    "\n",
    "def fqn(client: bigquery.Client, table: str) -> str:\n",
    "    return f\"{client.project}.{BQ_DATASET_ID}.{table}\"\n",
    "\n",
    "def ensure_dataset(client: bigquery.Client):\n",
    "    ds = bigquery.Dataset(f\"{client.project}.{BQ_DATASET_ID}\")\n",
    "    ds.location = BQ_LOCATION\n",
    "    try:\n",
    "        client.create_dataset(ds)\n",
    "        logger.info(f\"âœ… Created dataset {BQ_DATASET_ID}\")\n",
    "    except Conflict:\n",
    "        pass\n",
    "\n",
    "# ---------- 4) SCHEMAS (THEO áº¢NH) ----------\n",
    "# Táº¡o báº£ng náº¿u thiáº¿u, cÃ²n náº¿u Ä‘Ã£ cÃ³ -> giá»¯ nguyÃªn\n",
    "def ensure_table_schema_call_log(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"call_log\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"chargeTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"direction\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromNumber\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"toNumber\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"startTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"endTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"duration\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"billDuration\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"hangupCause\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"answerTime\",\"FLOAT64\"),\n",
    "            bigquery.SchemaField(\"fromUser__id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromUser__name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromGroup__id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            # ðŸ‘‰ ThÃªm dÃ²ng nÃ y:\n",
    "            bigquery.SchemaField(\"NgayTao\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\"]\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"NgayTao\",\n",
    "        )\n",
    "        client.create_table(t)\n",
    "\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "\n",
    "def ensure_table_schema_staff(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"staff\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"email\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"name\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_group(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"group\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"group_id\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_rank_single(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"rank_mapping\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"code\",         \"STRING\"),\n",
    "            bigquery.SchemaField(\"grade\",        \"STRING\"),\n",
    "            bigquery.SchemaField(\"target_day\",   \"INT64\"),\n",
    "            bigquery.SchemaField(\"target_week\",  \"INT64\"),\n",
    "            bigquery.SchemaField(\"target_month\", \"INT64\"),\n",
    "            bigquery.SchemaField(\"week_key\",     \"STRING\"),\n",
    "            bigquery.SchemaField(\"week_start\",   \"DATE\"),\n",
    "            bigquery.SchemaField(\"snapshot_at\",  \"TIMESTAMP\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY, field=\"week_start\",\n",
    "        )\n",
    "        t.clustering_fields = [\"code\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_customer_main(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"customer\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"assignedTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"phone\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"customField_0_val\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"NgayUpdate\",\"DATE\"),  # â† partition\n",
    "            bigquery.SchemaField(\"NgayAssign\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"_id\"]\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"NgayUpdate\",\n",
    "        )\n",
    "        # KHÃ”NG báº­t require_partition_filter Ä‘á»ƒ MERGE/APPEND linh hoáº¡t\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_customer_stg(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"stg_customer\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"assignedTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"phone\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"customField_0_val\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"NgayUpdate\",\"DATE\"),\n",
    "            bigquery.SchemaField(\"NgayAssign\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        # staging khÃ´ng cáº§n partition\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "\n",
    "# ---------- 5) UPDATE LOG ----------\n",
    "def ensure_update_log(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"update_log\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"table_name\",\"STRING\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"updated_at\",\"TIMESTAMP\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"rows_loaded\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"max_updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"mode\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.time_partitioning = bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field=\"updated_at\")\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def sanitize_table_name(name: str) -> str:\n",
    "    s = name.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return s\n",
    "\n",
    "# Cache checkpoint trong RAM\n",
    "CHECKPOINT: Dict[Tuple[str,str], Optional[int]] = {}\n",
    "LAST_RUN: Dict[Tuple[str,str], Optional[datetime]] = {}\n",
    "\n",
    "def warm_checkpoint_cache(client: bigquery.Client):\n",
    "    ensure_update_log(client)\n",
    "    sql = f\"\"\"\n",
    "      SELECT table_name, tenant, \n",
    "             MAX(max_updateTime) AS ck,\n",
    "             MAX(updated_at)     AS last_run_at\n",
    "      FROM `{fqn(client, \"update_log\")}`\n",
    "      GROUP BY table_name, tenant\n",
    "    \"\"\"\n",
    "    for r in client.query(sql, location=BQ_LOCATION).result():\n",
    "        tbl = str(r[\"table_name\"]).lower()\n",
    "        tnt = r[\"tenant\"]\n",
    "        CHECKPOINT[(tbl, tnt)] = int(r[\"ck\"]) if r[\"ck\"] is not None else None\n",
    "        dt = r[\"last_run_at\"]\n",
    "        if isinstance(dt, datetime) and dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        LAST_RUN[(tbl, tnt)] = dt\n",
    "    logger.info(f\"ðŸ”¥ Warmed checkpoint cache: {len(CHECKPOINT)} keys (with last_run)\")\n",
    "\n",
    "def get_last_run_any(table_name: str) -> Optional[datetime]:\n",
    "    \"\"\"Láº¥y láº§n cháº¡y gáº§n nháº¥t cá»§a má»™t báº£ng (gom má»i tenant).\"\"\"\n",
    "    tbl = sanitize_table_name(table_name)\n",
    "    dts = [dt for (t, _), dt in LAST_RUN.items() if t == tbl and dt is not None]\n",
    "    return max(dts) if dts else None\n",
    "\n",
    "def get_last_run_by_prefix(prefix: str) -> Optional[datetime]:\n",
    "    \"\"\"Láº¥y láº§n cháº¡y gáº§n nháº¥t cho cÃ¡c báº£ng cÃ³ tiá»n tá»‘ (vd rank_mapping_YYYYwWW).\"\"\"\n",
    "    pfx = sanitize_table_name(prefix)\n",
    "    dts = [dt for (t, _), dt in LAST_RUN.items() if t.startswith(pfx) and dt is not None]\n",
    "    return max(dts) if dts else None\n",
    "\n",
    "\n",
    "def get_ck(table: str, tenant: str) -> Optional[int]:\n",
    "    return CHECKPOINT.get((sanitize_table_name(table), tenant))\n",
    "\n",
    "def set_ck(table: str, tenant: str, value: Optional[int]):\n",
    "    CHECKPOINT[(sanitize_table_name(table), tenant)] = value\n",
    "\n",
    "# Buffer log, flush 1 phÃ¡t\n",
    "PENDING_LOGS: List[Dict[str, Any]] = []\n",
    "\n",
    "def add_log(tenant: str, table_name: str, rows: int, max_ut_ms: Optional[int], mode: str):\n",
    "    PENDING_LOGS.append({\n",
    "        \"table_name\": sanitize_table_name(table_name),\n",
    "        \"tenant\": tenant,\n",
    "        \"updated_at\": datetime.now(timezone.utc),\n",
    "        \"rows_loaded\": int(rows),\n",
    "        \"max_updateTime\": int(max_ut_ms) if max_ut_ms is not None else None,\n",
    "        \"mode\": mode,\n",
    "    })\n",
    "\n",
    "def flush_logs(client: bigquery.Client):\n",
    "    if not PENDING_LOGS: return\n",
    "    ensure_update_log(client)\n",
    "    df = pd.DataFrame(PENDING_LOGS)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        autodetect=False,\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client,\"update_log\"),\n",
    "                                     job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    logger.info(f\"ðŸ“ Flushed update_log: {len(PENDING_LOGS)} rows\")\n",
    "    PENDING_LOGS.clear()\n",
    "\n",
    "# ---------- 6) UTIL & TRANSFORM ----------\n",
    "def iso_week_key(dt: datetime) -> str:\n",
    "    # Tráº£ vá» dáº¡ng YYYYwWW (vd: 2025w36)\n",
    "    iso = dt.isocalendar()\n",
    "    try:\n",
    "        year, week = iso.year, iso.week\n",
    "    except AttributeError:\n",
    "        # Python cÅ© tráº£ tuple (year, week, weekday)\n",
    "        year, week = iso[0], iso[1]\n",
    "    return f\"{int(year)}w{int(week):02d}\"\n",
    "\n",
    "def week_start_vn(dt: datetime) -> datetime.date:\n",
    "    # TÃ­nh Ä‘áº§u tuáº§n theo VN (Thá»© 2) dÃ¹ng offset +7h cho cháº¯c\n",
    "    local = dt + timedelta(hours=7)\n",
    "    return (local.date() - timedelta(days=(local.isoweekday() - 1)))\n",
    "\n",
    "def safe_eval(v):\n",
    "    if isinstance(v, str):\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            try:\n",
    "                import ast; return ast.literal_eval(v)\n",
    "            except Exception:\n",
    "                return v\n",
    "    return v\n",
    "\n",
    "def derive_cf0_string_from_df(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"customFields\" not in df.columns:\n",
    "        return pd.Series([None]*len(df), dtype=\"string\")\n",
    "\n",
    "    def pick(x):\n",
    "        x = safe_eval(x)\n",
    "        first = None\n",
    "        if isinstance(x, list) and x: first = x[0]\n",
    "        elif isinstance(x, dict):     first = x\n",
    "        if first is None: return None\n",
    "\n",
    "        cand = None\n",
    "        if isinstance(first, dict):\n",
    "            for k in (\"val\",\"value\",\"values\",\"text\",\"name\"):\n",
    "                if k in first: cand = first[k]; break\n",
    "        else:\n",
    "            cand = first\n",
    "\n",
    "        vals = []\n",
    "        if cand is None: return None\n",
    "        if isinstance(cand, list):\n",
    "            for it in cand:\n",
    "                if isinstance(it, dict):\n",
    "                    v = it.get(\"value\") or it.get(\"name\") or it.get(\"text\")\n",
    "                else:\n",
    "                    v = it\n",
    "                if v is None: continue\n",
    "                s = str(v).strip()\n",
    "                if s and s.lower() != \"nan\": vals.append(s)\n",
    "        else:\n",
    "            s = str(cand).strip()\n",
    "            if s and s.lower() != \"nan\": vals.append(s)\n",
    "        if not vals: return None\n",
    "        # de-dup nhÆ°ng giá»¯ thá»© tá»±\n",
    "        seen=set(); out=[]\n",
    "        for v in vals:\n",
    "            if v in seen: continue\n",
    "            seen.add(v); out.append(v)\n",
    "        return \" | \".join(out)\n",
    "\n",
    "    return df[\"customFields\"].apply(pick).astype(\"string\")\n",
    "\n",
    "def extract_user_id(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict): return x.get(\"_id\") or x.get(\"id\")\n",
    "        if isinstance(x, (list,tuple)):\n",
    "            try: d=dict(x); return d.get(\"_id\") or d.get(\"id\")\n",
    "            except Exception: return None\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "\n",
    "def extract_user_name(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict): return x.get(\"name\")\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "\n",
    "def extract_user_group_id(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict):\n",
    "            g = x.get(\"group\")\n",
    "            if isinstance(g, dict): return g.get(\"_id\") or g.get(\"id\")\n",
    "            return g\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "    \n",
    "def compute_row_hash(df: pd.DataFrame) -> pd.Series:\n",
    "    if df.empty: return pd.Series([], dtype=\"string\")\n",
    "    volatile = {\"row_hash\",\"updateTime\",\"createTime\",\"updatedAt\",\"createdAt\",\n",
    "                \"NgayTao\",\"NgayUpdate\",\"NgayAssign\"}\n",
    "    cols = [c for c in df.columns if c not in volatile]\n",
    "    def _h(row):\n",
    "        return hashlib.md5(json.dumps({c: row.get(c) for c in cols}, sort_keys=True, default=str).encode(\"utf-8\")).hexdigest()\n",
    "    return df[cols].apply(lambda r: _h(r), axis=1)\n",
    "\n",
    "def yyyymm_from_ms(ms: Optional[int]) -> str:\n",
    "    if ms is None: \n",
    "        return datetime.now(timezone.utc).strftime(\"%Y%m\")\n",
    "    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).strftime(\"%Y%m\")\n",
    "\n",
    "def ms_to_iso(ms: Optional[int]) -> str:\n",
    "    if ms is None: return \"None\"\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(ms)/1000, tz=timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return str(ms)\n",
    "\n",
    "def pct(x: float) -> str:\n",
    "    try:\n",
    "        return f\"{max(0.0, min(1.0, x))*100:.1f}%\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ---------- 7) API ----------\n",
    "def callio_login(email: str, password: str) -> str:\n",
    "    r = requests.post(f\"{CALLIO_API_BASE_URL}/auth/login\",\n",
    "                      json={\"email\": email, \"password\": password}, timeout=API_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    tk = (r.json() or {}).get(\"token\")\n",
    "    if not tk: raise RuntimeError(\"Cannot get Callio token\")\n",
    "    return tk\n",
    "\n",
    "def fetch_desc_until(endpoint: str,\n",
    "                     tenant: str, email: str, password: str,\n",
    "                     time_field: str, cutoff_ms: int, log_prefix: str = \"\") -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Láº¥y theo DESC vÃ  dá»«ng khi record cÃ³ time_field <= cutoff_ms.\n",
    "    Tá»± refresh token vÃ  retry 1 láº§n náº¿u gáº·p 401.\n",
    "    Log tiáº¿n Ä‘á»™ tá»«ng trang: tá»•ng gom, last_ts, % phá»§ thá»i gian.\n",
    "    \"\"\"\n",
    "    token = get_token(tenant, email, password)\n",
    "    if not token:\n",
    "        raise RuntimeError(f\"[{tenant}] cannot obtain token\")\n",
    "\n",
    "    headers = {\"token\": token}\n",
    "    page, all_docs = 1, []\n",
    "    window_end_ms = int(time.time() * 1000)\n",
    "    denom = max(1, window_end_ms - int(cutoff_ms or 0))  # trÃ¡nh chia 0\n",
    "\n",
    "    while True:\n",
    "        params = {\"page\": page, \"pageSize\": API_PAGE_SIZE, \"sort\": f\"{time_field}DESC\"}\n",
    "\n",
    "        # --- request + retry 401 má»™t láº§n\n",
    "        try:\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/{endpoint}\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "            if r.status_code == 401:\n",
    "                logger.warning(f\"{log_prefix} 401 on page={page} â†’ refreshing token and retry once\")\n",
    "                token = get_token(tenant, email, password, force=True)\n",
    "                if not token:\n",
    "                    raise RuntimeError(f\"[{tenant}] token refresh failed\")\n",
    "                headers = {\"token\": token}\n",
    "                r = requests.get(f\"{CALLIO_API_BASE_URL}/{endpoint}\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "        except Exception as ex:\n",
    "            raise\n",
    "\n",
    "        r.raise_for_status()\n",
    "        data = r.json() or {}\n",
    "        docs = data.get(\"docs\") or []\n",
    "        total_docs = data.get(\"totalDocs\") or data.get(\"total\") or None\n",
    "        has_next = bool(data.get(\"hasNextPage\", False))\n",
    "        cur_count, stop_flag = 0, False\n",
    "\n",
    "        for d in docs:\n",
    "            ts = int(d.get(time_field) or 0)\n",
    "            if ts <= cutoff_ms:\n",
    "                stop_flag = True\n",
    "                break\n",
    "            all_docs.append(d)\n",
    "            cur_count += 1\n",
    "\n",
    "        last_ts = int(all_docs[-1].get(time_field)) if all_docs else None\n",
    "        progress = (window_end_ms - (last_ts or window_end_ms)) / denom if denom > 0 else 0.0\n",
    "\n",
    "        logger.info(\n",
    "            f\"{log_prefix} page={page} got={cur_count} cum={len(all_docs)} \"\n",
    "            f\"last_ts={ms_to_iso(last_ts)} window=[{ms_to_iso(cutoff_ms)} â†’ {ms_to_iso(window_end_ms)}] \"\n",
    "            f\"time_coverageâ‰ˆ{pct(progress)} totalDocs={total_docs} hasNext={has_next and not stop_flag}\"\n",
    "        )\n",
    "\n",
    "        if LIMIT_RECORDS_PER_ENDPOINT and len(all_docs) >= LIMIT_RECORDS_PER_ENDPOINT:\n",
    "            all_docs = all_docs[:LIMIT_RECORDS_PER_ENDPOINT]\n",
    "            logger.info(f\"{log_prefix} hit LIMIT_RECORDS_PER_ENDPOINT={LIMIT_RECORDS_PER_ENDPOINT}\")\n",
    "            break\n",
    "\n",
    "        if stop_flag or not has_next:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    logger.info(f\"{log_prefix} DONE pages={page} loaded={len(all_docs)} \"\n",
    "                f\"range=[{ms_to_iso(cutoff_ms)} â†’ {ms_to_iso(all_docs[0].get(time_field) if all_docs else None)}]\")\n",
    "    return all_docs\n",
    "\n",
    "# ---------- 8) LOADERS ----------\n",
    "def ensure_unique_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    seen, out = {}, []\n",
    "    for c in df.columns:\n",
    "        if c in seen:\n",
    "            seen[c]+=1; out.append(f\"{c}__{seen[c]}\")\n",
    "        else:\n",
    "            seen[c]=0;  out.append(c)\n",
    "    df.columns = out\n",
    "    return df\n",
    "\n",
    "def load_append(client: bigquery.Client, df: pd.DataFrame, table: str):\n",
    "    if df.empty: return 0\n",
    "    df = ensure_unique_columns(df)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        autodetect=True,\n",
    "        schema_update_options=[\n",
    "            bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,\n",
    "            bigquery.SchemaUpdateOption.ALLOW_FIELD_RELAXATION\n",
    "        ],\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client, table), job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    return len(df)\n",
    "\n",
    "def load_truncate(client: bigquery.Client, df: pd.DataFrame, table: str):\n",
    "    if df.empty: return 0\n",
    "    df = ensure_unique_columns(df)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True,\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client, table), job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    return len(df)\n",
    "\n",
    "def _strip_accents_upper(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", str(s or \"\"))\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "    return s.upper().strip()\n",
    "\n",
    "def _extract_code(token: str) -> Optional[str]:\n",
    "    s = _strip_accents_upper(token)\n",
    "    m = re.search(r\"\\b([A-Z]{2}\\d{2})\\b\", s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _clean_grade(token: str) -> Optional[str]:\n",
    "    s = _strip_accents_upper(token)\n",
    "    m = re.search(r\"([A-Z])\", s)\n",
    "    return m.group(1) if m else (s or None)\n",
    "\n",
    "def _df_from_block(rows: List[List[Any]]) -> pd.DataFrame:\n",
    "    # Tráº£ vá»: code, grade, target_day, target_week, target_month\n",
    "    if not rows or len(rows) < 2:\n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\"])\n",
    "\n",
    "    # âœ… Header = dÃ²ng 1\n",
    "    header = rows[0]\n",
    "    body   = rows[1:]\n",
    "\n",
    "    # Chuáº©n hoÃ¡ Ä‘á»™ rá»™ng\n",
    "    width_body = max((len(r) for r in body), default=len(header))\n",
    "    width = max(len(header), width_body)\n",
    "    header = (header + [None]*(width - len(header)))[:width]\n",
    "    body_norm = [(r + [None]*(width - len(r)))[:width] for r in body]\n",
    "\n",
    "    def norm(s): return _strip_accents_upper(s or \"\")\n",
    "    H = [norm(x) for x in header]\n",
    "\n",
    "    # TÃ¬m cá»™t theo tÃªn (Æ°u tiÃªn tiáº¿ng Viá»‡t)\n",
    "    def find_col(words_any=None, must_have=None):\n",
    "        def norm_tok(x): return _strip_accents_upper(x or \"\")\n",
    "        for i, h in enumerate(H):  # H Ä‘Ã£ lÃ  header normalize\n",
    "            ok = True\n",
    "            if must_have:\n",
    "                ok = all(norm_tok(m) in h for m in must_have)\n",
    "            if ok and words_any:\n",
    "                ok = any(norm_tok(w) in h for w in words_any)\n",
    "            if ok: return i\n",
    "        return None\n",
    "\n",
    "    # mapping tÃªn cá»™t\n",
    "    code_idx  = find_col(words_any=(\"MA NV\",\"MA NHAN VIEN\",\"NHAN VIEN\",\"MA\",\"CODE\"))\n",
    "    grade_idx = find_col(words_any=(\"HANG\",\"CAP\",\"BAC\",\"RANK\",\"GRADE\"))\n",
    "    day_idx   = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"NGAY\",))\n",
    "    week_idx  = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"TUAN\",))\n",
    "    month_idx = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"THANG\",))\n",
    "\n",
    "    df = pd.DataFrame(body_norm, columns=range(width))\n",
    "\n",
    "    # fallback theo ná»™i dung náº¿u header khÃ´ng rÃµ\n",
    "    if code_idx is None:\n",
    "        hits = {i: df[i].astype(str).map(_extract_code).notna().sum() for i in range(width)}\n",
    "        code_idx = max(hits, key=hits.get) if hits and max(hits.values())>0 else None\n",
    "    if grade_idx is None:\n",
    "        hits = {i: df[i].astype(str).map(_clean_grade).notna().sum() for i in range(width)}\n",
    "        grade_idx = max(hits, key=hits.get) if hits and max(hits.values())>0 else None\n",
    "\n",
    "    if code_idx is None:\n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\"])\n",
    "\n",
    "    def to_int(v):\n",
    "        s = str(v or \"\").strip()\n",
    "        s = s.replace(\" \", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "        m = re.findall(r\"\\d+\", s)\n",
    "        return int(\"\".join(m)) if m else None\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"code\":         df[code_idx].map(_extract_code),\n",
    "        \"grade\":        df[grade_idx].map(_clean_grade) if grade_idx is not None else None,\n",
    "        \"target_day\":   df[day_idx].map(to_int)   if day_idx   is not None else None,\n",
    "        \"target_week\":  df[week_idx].map(to_int)  if week_idx  is not None else None,\n",
    "        \"target_month\": df[month_idx].map(to_int) if month_idx is not None else None,\n",
    "    })\n",
    "    out = out[out[\"code\"].notna()].drop_duplicates(\"code\", keep=\"first\")\n",
    "    return out\n",
    "\n",
    "def read_rank_mapping() -> pd.DataFrame:\n",
    "    info = json.loads(SERVICE_ACCOUNT_KEY_JSON)\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
    "    creds = gsa.Credentials.from_service_account_info(info, scopes=scopes)\n",
    "    gc = gspread.authorize(creds)\n",
    "    ws = gc.open_by_key(RANK_SHEET_ID).worksheet(RANK_TAB_NAME)\n",
    "    parts = []\n",
    "    for rng in [r.strip() for r in RANK_RANGES_A1 if r.strip()]:\n",
    "        rows = ws.get(rng)\n",
    "        dfp = _df_from_block(rows)\n",
    "        if not dfp.empty:\n",
    "            parts.append(dfp)\n",
    "    if not parts: \n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\"])\n",
    "    all_df = pd.concat(parts, ignore_index=True).drop_duplicates(\"code\", keep=\"first\")\n",
    "    return all_df\n",
    "\n",
    "def run_rank_mapping_weekly(client: bigquery.Client, weekly_key: str):\n",
    "    # 1) Äá»c sheet & chuáº©n hÃ³a\n",
    "    df_map = read_rank_mapping()\n",
    "    if df_map.empty:\n",
    "        logger.warning(\"âš ï¸ rank_mapping trá»‘ng â€“ bá» qua\")\n",
    "        return 0\n",
    "\n",
    "    # 2) Äáº£m báº£o báº£ng Ä‘Ã­ch (schema khÃ´ng cÃ³ grade_value)\n",
    "    ensure_table_schema_rank_single(client)\n",
    "\n",
    "    # 3) Chuáº©n hÃ³a nháº¹ cÃ¡c cá»™t hiá»‡n cÃ³\n",
    "    df_map = df_map.copy()\n",
    "    if \"grade\" in df_map.columns:\n",
    "        df_map[\"grade\"] = df_map[\"grade\"].astype(\"string\").str.upper().str.strip()\n",
    "    if \"code\" in df_map.columns:\n",
    "        df_map[\"code\"] = df_map[\"code\"].astype(\"string\").str.upper().str.strip()\n",
    "\n",
    "    # 4) Gáº¯n key tuáº§n & timestamp\n",
    "    now = datetime.now(timezone.utc)\n",
    "    ws = week_start_vn(now)  # DATE (thá»© 2 VN)\n",
    "    df_map[\"week_key\"]    = weekly_key\n",
    "    df_map[\"week_start\"]  = ws\n",
    "    df_map[\"snapshot_at\"] = pd.Timestamp.utcnow()\n",
    "\n",
    "    # 5) XoÃ¡ dá»¯ liá»‡u tuáº§n nÃ y rá»“i append\n",
    "    full_tbl = fqn(client, \"rank_mapping\")\n",
    "    del_sql = f\"DELETE FROM `{full_tbl}` WHERE week_start = @ws\"\n",
    "    job_cfg = bigquery.QueryJobConfig(\n",
    "        query_parameters=[bigquery.ScalarQueryParameter(\"ws\", \"DATE\", ws)]\n",
    "    )\n",
    "    client.query(del_sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    # 6) Chá»‰ giá»¯ cÃ¡c cá»™t cáº§n thiáº¿t khi load (phÃ²ng khi read_rank_mapping tráº£ nhiá»u cá»™t)\n",
    "    wanted = [c for c in [\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\",\n",
    "                          \"week_key\",\"week_start\",\"snapshot_at\"] if c in df_map.columns]\n",
    "    rows = load_append(client, df_map[wanted], \"rank_mapping\")\n",
    "    logger.info(f\"[rank_mapping] APPEND rows={rows} for week={weekly_key} (week_start={ws})\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 9) FLOWS ----------\n",
    "def run_customer_for_tenant(client: bigquery.Client, token_unused: str, tenant: str):\n",
    "    table_key = \"customer\"\n",
    "    ck = get_ck(table_key, tenant)\n",
    "    if ck is None:\n",
    "        ck = int((datetime.now(timezone.utc) - timedelta(days=DAYS_TO_FETCH_IF_EMPTY)).timestamp()*1000)\n",
    "    cutoff = ck - OVERLAP_MS if ck else ck\n",
    "\n",
    "    logger.info(f\"[{tenant}][customer] ck={ck} ({ms_to_iso(ck)}) overlap={OVERLAP_MS} â†’ cutoff={ms_to_iso(cutoff)}\")\n",
    "\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    docs = fetch_desc_until(\"customer\", tenant, email, password, \"updateTime\", cutoff, log_prefix=f\"[{tenant}][customer]\")\n",
    "    if not docs:\n",
    "        # NOOP: khÃ´ng Ä‘á»•i CK, nhÆ°ng váº«n cÃ³ thá»ƒ muá»‘n nhÃ¬n tháº¥y log\n",
    "        add_log(tenant, table_key, 0, None, \"NOOP\")\n",
    "        return (None, None), 0, ck, None\n",
    "\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    df[\"user_id\"]       = extract_user_id(df)\n",
    "    df[\"user_name\"]     = extract_user_name(df)\n",
    "    df[\"user_group_id\"] = extract_user_group_id(df)\n",
    "\n",
    "    if \"customField_0_val\" not in df.columns:\n",
    "        df[\"customField_0_val\"] = derive_cf0_string_from_df(df)\n",
    "    df[\"customField_0_val\"] = df[\"customField_0_val\"].astype(\"string\")\n",
    "\n",
    "    keep = [\n",
    "        \"_id\",\"assignedTime\",\"createTime\",\"updateTime\",\"name\",\"phone\",\n",
    "        \"user_id\",\"user_name\",\"user_group_id\",\n",
    "        \"tenant\",\"row_hash\",\"customField_0_val\",\n",
    "        \"NgayUpdate\",\"NgayAssign\"\n",
    "    ]\n",
    "    for c in keep:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    out = df[keep].copy()\n",
    "    out[\"tenant\"]      = tenant\n",
    "    out[\"NgayUpdate\"]  = pd.to_datetime(pd.to_numeric(out[\"updateTime\"], errors=\"coerce\"), unit=\"ms\", utc=True).dt.date\n",
    "    out[\"NgayAssign\"]  = pd.to_datetime(pd.to_numeric(out[\"assignedTime\"], errors=\"coerce\"), unit=\"ms\", utc=True).dt.date\n",
    "    out[\"row_hash\"]    = compute_row_hash(out)\n",
    "\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "    rows = load_append(client, out, \"stg_customer\")\n",
    "\n",
    "    # max_ut theo tenant â€“ sáº½ dÃ¹ng Ä‘á»ƒ set CK SAU MERGE\n",
    "    max_ut = int(pd.to_numeric(out[\"updateTime\"], errors=\"coerce\").max()) if rows else None\n",
    "\n",
    "    # Log STAGED nhÆ°ng KHÃ”NG Ä‘Ã­nh kÃ¨m max_updateTime (Ä‘á»ƒ khÃ´ng kÃ©o CK)\n",
    "    d_min = pd.to_datetime(out[\"NgayUpdate\"]).min()\n",
    "    d_max = pd.to_datetime(out[\"NgayUpdate\"]).max()\n",
    "    add_log(tenant, table_key, int(rows), None, \"STAGED\")\n",
    "    logger.info(f\"[{tenant}][customer] STAGED rows={rows} window=[{d_min.date()}..{d_max.date()}] (ck unchanged)\")\n",
    "    return (d_min.date(), d_max.date()), int(rows), ck, max_ut\n",
    "\n",
    "\n",
    "def merge_customer_window(client: bigquery.Client, d_from: datetime.date, d_to: datetime.date):\n",
    "    \"\"\"MERGE stg_customer -> customer, chá»‰ scan cÃ¡c partition trong [d_from..d_to].\"\"\"\n",
    "    ensure_table_schema_customer_main(client)\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "\n",
    "    full_stg = fqn(client, \"stg_customer\")\n",
    "    full_tgt = fqn(client, \"customer\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    DECLARE d_from DATE DEFAULT @d_from;\n",
    "    DECLARE d_to   DATE DEFAULT @d_to;\n",
    "\n",
    "    -- Lá»c staging theo window\n",
    "    CREATE TEMP TABLE _S AS\n",
    "    SELECT *\n",
    "    FROM `{full_stg}`\n",
    "    WHERE NgayUpdate BETWEEN d_from AND d_to;\n",
    "\n",
    "    -- MERGE cÃ³ filter partition á»Ÿ ON (T.NgayUpdate BETWEEN ...)\n",
    "    MERGE `{full_tgt}` T\n",
    "    USING (\n",
    "      SELECT tenant, `_id`, assignedTime, createTime, updateTime, name, phone,\n",
    "             user_id, user_name, user_group_id,\n",
    "             row_hash, customField_0_val, NgayUpdate, NgayAssign\n",
    "      FROM _S\n",
    "      QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY tenant, `_id`\n",
    "        ORDER BY SAFE_CAST(updateTime AS INT64) DESC\n",
    "      ) = 1\n",
    "    ) S\n",
    "    ON T.tenant = S.tenant\n",
    "       AND T._id = S._id\n",
    "       AND T.NgayUpdate BETWEEN d_from AND d_to\n",
    "    WHEN MATCHED AND (T.row_hash IS NULL OR T.row_hash != S.row_hash\n",
    "                      OR SAFE_CAST(S.updateTime AS INT64) >= SAFE_CAST(T.updateTime AS INT64)\n",
    "                      OR T.updateTime IS NULL) THEN\n",
    "      UPDATE SET\n",
    "        assignedTime = S.assignedTime,\n",
    "        createTime   = S.createTime,\n",
    "        updateTime   = S.updateTime,\n",
    "        name         = S.name,\n",
    "        phone        = S.phone,\n",
    "        user_id      = S.user_id,\n",
    "        user_name    = S.user_name,\n",
    "        user_group_id= S.user_group_id,\n",
    "        row_hash     = S.row_hash,\n",
    "        customField_0_val = S.customField_0_val,\n",
    "        NgayUpdate   = S.NgayUpdate,\n",
    "        NgayAssign   = S.NgayAssign\n",
    "    WHEN NOT MATCHED BY TARGET THEN\n",
    "      INSERT (tenant, _id, assignedTime, createTime, updateTime, name, phone, user_id,\n",
    "              user_name, user_group_id, row_hash, customField_0_val, NgayUpdate, NgayAssign)\n",
    "      VALUES (S.tenant, S._id, S.assignedTime, S.createTime, S.updateTime, S.name, S.phone, S.user_id,\n",
    "              S.user_name, S.user_group_id, S.row_hash, S.customField_0_val, S.NgayUpdate, S.NgayAssign);\n",
    "    \"\"\"\n",
    "    job_cfg = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"d_from\", \"DATE\", d_from),\n",
    "            bigquery.ScalarQueryParameter(\"d_to\",   \"DATE\", d_to),\n",
    "        ]\n",
    "    )\n",
    "    client.query(sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    # Dá»n staging theo window Ä‘á»ƒ nháº¹ báº£ng stg\n",
    "    del_sql = f\"DELETE FROM `{full_stg}` WHERE NgayUpdate BETWEEN @d_from AND @d_to\"\n",
    "    client.query(del_sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    logger.info(f\"ðŸ§© MERGE customer done for window [{d_from}..{d_to}] & cleaned staging.\")\n",
    "\n",
    "def run_call_for_tenant(client: bigquery.Client, token_unused: str, tenant: str, email: Optional[str] = None, password: Optional[str] = None):\n",
    "    table_key = \"call_log\"\n",
    "    ck = get_ck(table_key, tenant)\n",
    "    if ck is None:\n",
    "        ck = int((datetime.now(timezone.utc) - timedelta(days=DAYS_TO_FETCH_IF_EMPTY)).timestamp()*1000)\n",
    "    # â— KhÃ´ng overlap Ä‘á»ƒ trÃ¡nh trÃ¹ng\n",
    "    cutoff = ck  # láº¥y strictly createTime > ck\n",
    "    logger.info(f\"[{tenant}][call_log] ck={ck} ({ms_to_iso(ck)}) \"\n",
    "                f\"â†’ cutoff={cutoff} ({ms_to_iso(cutoff)}); window_end=now={ms_to_iso(int(time.time()*1000))}\")\n",
    "\n",
    "    # Láº¥y email/password tá»« ACCOUNTS náº¿u khÃ´ng truyá»n\n",
    "    if (email is None) or (password is None):\n",
    "        acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "        if acc:\n",
    "            email, password = acc[\"email\"], acc[\"password\"]\n",
    "\n",
    "    try:\n",
    "        docs = fetch_desc_until(\"call\", tenant, email, password, \"createTime\", cutoff, log_prefix=f\"[{tenant}][call_log]\")\n",
    "    except requests.HTTPError as http_err:\n",
    "        if getattr(http_err.response, \"status_code\", None) == 401:\n",
    "            add_log(tenant, table_key, 0, ck, \"ERROR_401\")\n",
    "            logger.error(f\"[{tenant}][call_log] 401 after retry â€” check credentials/token scope\")\n",
    "        raise\n",
    "    if not docs:\n",
    "        add_log(tenant, table_key, 0, ck, \"NOOP\"); \n",
    "        logger.info(f\"[{tenant}][call_log] NOOP (no new docs)\"); \n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(docs)\n",
    "    if not df.empty and \"_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"_id\"], keep=\"last\")\n",
    "    keep_cols = [\"_id\",\"chargeTime\",\"createTime\",\"direction\",\"fromNumber\",\"toNumber\",\n",
    "             \"startTime\",\"endTime\",\"duration\",\"billDuration\",\"hangupCause\",\n",
    "             \"answerTime\"]\n",
    "    out = pd.DataFrame()\n",
    "    for c in keep_cols:\n",
    "        out[c] = df.get(c, pd.Series([None]*len(df)))\n",
    "    # fromUser (id + name)\n",
    "    if \"fromUser\" in df.columns:\n",
    "        out[\"fromUser__id\"] = df[\"fromUser\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "        out[\"fromUser__name\"] = df[\"fromUser\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"name\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"fromUser__id\"] = None\n",
    "        out[\"fromUser__name\"] = None\n",
    " \n",
    "    # fromGroup (only id)\n",
    "    if \"fromGroup\" in df.columns:\n",
    "        out[\"fromGroup__id\"] = df[\"fromGroup\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"fromGroup__id\"] = None\n",
    "\n",
    "    out[\"NgayTao\"] = pd.to_datetime(\n",
    "        pd.to_numeric(out[\"createTime\"], errors=\"coerce\"),\n",
    "        unit=\"ms\", utc=True\n",
    "    ).dt.date\n",
    "        \n",
    "    out[\"tenant\"] = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "\n",
    "    ensure_table_schema_call_log(client)\n",
    "    loaded = load_append(client, out, \"call_log\")\n",
    "    max_ct = int(out[\"createTime\"].max()) if \"createTime\" in out.columns and not out[\"createTime\"].empty else ck\n",
    "    if max_ct is not None and (get_ck(table_key, tenant) is None or max_ct > get_ck(table_key, tenant)):\n",
    "        set_ck(table_key, tenant, max_ct)\n",
    "    add_log(tenant, table_key, loaded, get_ck(table_key, tenant), \"APPEND\")\n",
    "    logger.info(f\"[{tenant}][call_log] APPEND loaded={loaded} new_ck={get_ck(table_key, tenant)} ({ms_to_iso(get_ck(table_key, tenant))})\")\n",
    "\n",
    "def run_staff_for_tenant(client: bigquery.Client, token_unused: str, tenant: str):\n",
    "    \"\"\"\n",
    "    Fetch staff for a tenant (Callio /user), chuáº©n hÃ³a tá»‘i thiá»ƒu, \n",
    "    KHÃ”NG sinh workingTime, notifyToBrowserAudio. \n",
    "    Tráº£ vá» DataFrame cÃ³ cÃ¡c cá»™t:\n",
    "      _id, email, name, updateTime, createTime, group_id, tenant, row_hash\n",
    "    \"\"\"\n",
    "    # Láº¥y credential theo tenant\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    # Call API + retry 401 1 láº§n\n",
    "    tk = get_token(tenant, email, password)\n",
    "    r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers={\"token\": tk}, timeout=API_TIMEOUT)\n",
    "    if r.status_code == 401:\n",
    "        logger.warning(f\"[{tenant}][staff] 401 â†’ refreshing token and retry\")\n",
    "        tk = get_token(tenant, email, password, force=True)\n",
    "        r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers={\"token\": tk}, timeout=API_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # Chuáº©n hÃ³a docs -> DataFrame\n",
    "    docs = (r.json() or {}).get(\"docs\") or r.json() or []\n",
    "    if not isinstance(docs, list):\n",
    "        docs = []\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    if df.empty:\n",
    "        add_log(tenant, \"staff\", 0, None, \"NOOP\")\n",
    "        return df\n",
    "\n",
    "    # Chá»‰ giá»¯ cá»™t cáº§n thiáº¿t\n",
    "    out = pd.DataFrame()\n",
    "    out[\"_id\"]        = df.get(\"_id\", None)\n",
    "    out[\"email\"]      = df.get(\"email\", None)\n",
    "    out[\"name\"]       = df.get(\"name\", None)\n",
    "    out[\"updateTime\"] = df.get(\"updateTime\", None)\n",
    "    out[\"createTime\"] = df.get(\"createTime\", None)\n",
    "\n",
    "    # group_id (giáº£i nÃ©n tá»« object 'group')\n",
    "    if \"group\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"group\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"group_id\"] = None\n",
    "\n",
    "    # Bá»• sung metadata\n",
    "    out[\"tenant\"]   = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def run_group_for_tenant(client: bigquery.Client, token_unused: str, tenant: str):\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    tk = get_token(tenant, email, password)\n",
    "    docs = None\n",
    "    for ep in (\"group\",\"user-group\"):\n",
    "        try:\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/{ep}\", headers={\"token\":tk}, timeout=API_TIMEOUT)\n",
    "            if r.status_code == 401:\n",
    "                logger.warning(f\"[{tenant}][{ep}] 401 â†’ refreshing token and retry\")\n",
    "                tk = get_token(tenant, email, password, force=True)\n",
    "                r = requests.get(f\"{CALLIO_API_BASE_URL}/{ep}\", headers={\"token\":tk}, timeout=API_TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            d = r.json() or {}\n",
    "            docs = d.get(\"docs\") or d\n",
    "            if isinstance(docs, list): break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not isinstance(docs, list): docs=[]\n",
    "    df = pd.DataFrame(docs)\n",
    "    out = pd.DataFrame()\n",
    "    if \"_id\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"_id\"].astype(\"string\")\n",
    "    elif \"id\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"id\"].astype(\"string\")\n",
    "    else:\n",
    "        out[\"group_id\"] = None\n",
    "    out[\"name\"] = df.get(\"name\", None)\n",
    "    out[\"tenant\"] = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "    return out\n",
    "\n",
    "# ---------- 10) SCHEDULER ----------\n",
    "def merge_staff_from_staging(client: bigquery.Client, stg_name: str = \"stg_staff\", tgt_name: str = \"staff\"):\n",
    "    full_stg = fqn(client, stg_name)\n",
    "    full_tgt = fqn(client, tgt_name)\n",
    "\n",
    "    # Láº¥y schema staging; náº¿u chÆ°a cÃ³ thÃ¬ thÃ´i\n",
    "    try:\n",
    "        stg_tbl = client.get_table(full_stg)\n",
    "    except NotFound:\n",
    "        logger.info(\"â„¹ï¸ No staged staff to merge.\"); \n",
    "        return\n",
    "\n",
    "    stg_cols = [f.name for f in stg_tbl.schema]\n",
    "    # Äáº£m báº£o cÃ³ cá»™t name (khÃ³a)\n",
    "    if \"name\" not in stg_cols or \"tenant\" not in stg_cols:\n",
    "        logger.warning(f\"âš ï¸ Staging {full_stg} thiáº¿u cá»™t khÃ³a (tenant/name) â€” bá» qua MERGE.\")\n",
    "        client.delete_table(full_stg, not_found_ok=True)\n",
    "        return\n",
    "\n",
    "    # Táº¡o target náº¿u thiáº¿u (clone schema láº§n Ä‘áº§u)\n",
    "    try:\n",
    "        tgt_tbl = client.get_table(full_tgt)\n",
    "    except NotFound:\n",
    "        t = bigquery.Table(full_tgt, schema=stg_tbl.schema)\n",
    "        t.clustering_fields = [\"tenant\",\"name\"]\n",
    "        client.create_table(t)\n",
    "        tgt_tbl = client.get_table(full_tgt)\n",
    "        logger.info(f\"âœ… Created target: {full_tgt}\")\n",
    "\n",
    "    tgt_cols = [f.name for f in tgt_tbl.schema]\n",
    "\n",
    "    # Giao cá»™t Ä‘á»ƒ UPDATE/INSERT (trá»« khÃ³a)\n",
    "    exclude = {\"tenant\",\"name\"}\n",
    "    cols_common = sorted((set(stg_cols) & set(tgt_cols)) - exclude)\n",
    "\n",
    "    # CÃ¡c cá»™t optional cho Ä‘iá»u kiá»‡n UPDATE\n",
    "    has_hash = \"row_hash\" in stg_cols and \"row_hash\" in tgt_cols\n",
    "    stg_has_upd = \"updateTime\" in stg_cols\n",
    "    tgt_has_upd = \"updateTime\" in tgt_cols\n",
    "\n",
    "    conds = []\n",
    "    if has_hash:\n",
    "        conds.append(\"(T.row_hash IS NULL OR T.row_hash != S.row_hash)\")\n",
    "    if stg_has_upd and tgt_has_upd:\n",
    "        conds.append(\"(SAFE_CAST(S.updateTime AS INT64) >= SAFE_CAST(T.updateTime AS INT64) OR T.updateTime IS NULL)\")\n",
    "    when_matched = \" AND \".join(conds) if conds else \"TRUE\"\n",
    "\n",
    "    # Chá»‰ select cÃ¡c cá»™t cáº§n thiáº¿t tá»« staging\n",
    "    proj_cols = [\"tenant\",\"name\"]\n",
    "    if has_hash: proj_cols.append(\"row_hash\")\n",
    "    if stg_has_upd: proj_cols.append(\"updateTime\")\n",
    "    for c in cols_common:\n",
    "        if c not in proj_cols: proj_cols.append(c)\n",
    "    proj_csv = \", \".join([f\"`{c}`\" for c in proj_cols])\n",
    "\n",
    "    set_clause   = \", \".join([f\"T.`{c}` = S.`{c}`\" for c in cols_common]) or \"T.`name` = T.`name`\"\n",
    "    insert_cols  = [\"tenant\",\"name\"] + cols_common\n",
    "    insert_vals  = [\"S.`tenant`\",\"S.`name`\"] + [f\"S.`{c}`\" for c in cols_common]\n",
    "\n",
    "    order_expr = \"SAFE_CAST(updateTime AS INT64) DESC\" if stg_has_upd else \"name\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    MERGE `{full_tgt}` T\n",
    "    USING (\n",
    "      SELECT {proj_csv}\n",
    "      FROM `{full_stg}`\n",
    "      QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY tenant, name\n",
    "        ORDER BY {order_expr}\n",
    "      ) = 1\n",
    "    ) S\n",
    "    ON T.tenant = S.tenant AND T.name = S.name\n",
    "    WHEN MATCHED AND ({when_matched}) THEN\n",
    "      UPDATE SET {set_clause}\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT ({\", \".join([f\"`{c}`\" for c in insert_cols])})\n",
    "      VALUES ({\", \".join(insert_vals)})\n",
    "    \"\"\"\n",
    "    client.query(sql, location=BQ_LOCATION).result()\n",
    "    logger.info(f\"ðŸ§© MERGE done: {full_stg} â†’ {full_tgt}\")\n",
    "\n",
    "    # Dá»n staging\n",
    "    client.delete_table(full_stg, not_found_ok=True)\n",
    "def next_daily(base_dt: datetime, hour_utc: int) -> datetime:\n",
    "    x = base_dt.replace(minute=0, second=0, microsecond=0)\n",
    "    tgt = x.replace(hour=hour_utc)\n",
    "    if tgt <= base_dt: tgt = tgt + timedelta(days=1)\n",
    "    return tgt\n",
    "\n",
    "def next_weekly(base_dt: datetime, dow: int, hour_utc: int) -> datetime:\n",
    "    # Mon=1..Sun=7\n",
    "    cur_dow = ((base_dt.isoweekday()-1) % 7) + 1\n",
    "    days_ahead = (dow - cur_dow) % 7\n",
    "    tgt = base_dt.replace(minute=0, second=0, microsecond=0, hour=hour_utc) + timedelta(days=days_ahead)\n",
    "    if tgt <= base_dt: tgt += timedelta(days=7)\n",
    "    return tgt\n",
    "\n",
    "# ---------- 11) MAIN LOOP (refactor cho cron) ----------\n",
    "def bootstrap():\n",
    "    client = get_bq_client()\n",
    "    ensure_dataset(client)\n",
    "    ensure_update_log(client)\n",
    "    ensure_table_schema_call_log(client)\n",
    "    ensure_table_schema_staff(client)\n",
    "    ensure_table_schema_group(client)\n",
    "    ensure_table_schema_customer_main(client)\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "    ensure_table_schema_rank_single(client)  # ðŸ‘ˆ thÃªm dÃ²ng nÃ y\n",
    "    warm_checkpoint_cache(client)\n",
    "    return client\n",
    "\n",
    "# Token cache (giá»¯ nguyÃªn)\n",
    "TOKENS: Dict[str, Tuple[str, float]] = {}  # tenant -> (token, expire_epoch)\n",
    "\n",
    "def get_token(tenant, email, password, force: bool = False) -> Optional[str]:\n",
    "    now = time.time()\n",
    "    tk, exp = TOKENS.get(tenant, (None, 0))\n",
    "    if (not force) and tk and now < exp:\n",
    "        return tk\n",
    "    try:\n",
    "        new_tk = callio_login(email, password)\n",
    "        TOKENS[tenant] = (new_tk, now + 25*60)  # TTL 25 phÃºt\n",
    "        logger.info(f\"[{tenant}] token {'refreshed' if force else 'obtained'}\")\n",
    "        return new_tk\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[{tenant}] login error: {e}\")\n",
    "        return None\n",
    "\n",
    "def snapshot_staff_group(client: bigquery.Client):\n",
    "    # ===== STAFF =====\n",
    "    staff_all = []\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        token = get_token(tnt, email, pwd)\n",
    "        if not token:\n",
    "            add_log(tnt, \"staff\", 0, None, \"ERROR_LOGIN\")\n",
    "            continue\n",
    "        try:\n",
    "            df = run_staff_for_tenant(client, token, tnt)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                staff_all.append(df)\n",
    "        except Exception:\n",
    "            logger.exception(f\"[{tnt}] staff error\")\n",
    "\n",
    "    if staff_all:\n",
    "        df_all = pd.concat(staff_all, ignore_index=True)\n",
    "        if \"name\" in df_all.columns:\n",
    "            df_all = df_all[df_all[\"name\"].notna() & (df_all[\"name\"].astype(str).str.strip() != \"\")]\n",
    "        else:\n",
    "            logger.warning(\"[staff] KhÃ´ng cÃ³ cá»™t 'name' â€” bá» qua load/merge.\")\n",
    "            add_log(\"ALL\", \"staff\", 0, None, \"NOOP\")\n",
    "            df_all = pd.DataFrame()\n",
    "\n",
    "        if not df_all.empty:\n",
    "            rows = load_append(client, df_all, \"stg_staff\")\n",
    "            add_log(\"ALL\", \"staff\", rows, None, \"STAGED\")\n",
    "            try:\n",
    "                merge_staff_from_staging(client, stg_name=\"stg_staff\", tgt_name=\"staff\")\n",
    "                add_log(\"ALL\", \"staff\", rows, None, \"MERGED\")\n",
    "            except Exception:\n",
    "                logger.exception(\"[staff] MERGE failed\")\n",
    "    else:\n",
    "        add_log(\"ALL\", \"staff\", 0, None, \"NOOP\")\n",
    "        logger.info(\"[staff] NOOP (no rows)\")\n",
    "\n",
    "    # ===== GROUP =====\n",
    "    grp_all = []\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        token = get_token(tnt, email, pwd)\n",
    "        if not token:\n",
    "            add_log(tnt, \"group\", 0, None, \"ERROR_LOGIN\")\n",
    "            continue\n",
    "        try:\n",
    "            df = run_group_for_tenant(client, token, tnt)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                grp_all.append(df)\n",
    "        except Exception:\n",
    "            logger.exception(f\"[{tnt}] group error\")\n",
    "\n",
    "    ensure_table_schema_group(client)\n",
    "    if grp_all:\n",
    "        df_all_grp = pd.concat(grp_all, ignore_index=True)\n",
    "        rows = load_truncate(client, df_all_grp, \"group\")   # snapshot háº±ng ngÃ y\n",
    "        add_log(\"ALL\", \"group\", rows, None, \"TRUNCATE\")\n",
    "        logger.info(f\"[group] TRUNCATE rows={rows}\")\n",
    "    else:\n",
    "        add_log(\"ALL\", \"group\", 0, None, \"NOOP\")\n",
    "        logger.info(\"[group] NOOP (no rows)\")\n",
    "\n",
    "\n",
    "def run_tick(client: bigquery.Client,\n",
    "             next_customer, next_call, next_staffgrp, next_rank):\n",
    "    loop_start = datetime.now(timezone.utc)\n",
    "\n",
    "    # --- CUSTOMER (15')\n",
    "    if loop_start >= next_customer:\n",
    "        logger.info(f\"â–¶ Run customer (all tenants) | interval={INTERVAL_CUSTOMER_MIN}m\")\n",
    "\n",
    "        win_min, win_max = None, None\n",
    "        staged_stats: Dict[str, Dict[str, Any]] = {}  # {tenant: {\"rows\": int, \"max_ut\": int|None}}\n",
    "\n",
    "        for acc in ACCOUNTS:\n",
    "            tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "            token = get_token(tnt, email, pwd)\n",
    "            if not token:\n",
    "                add_log(tnt, \"customer\", 0, None, \"ERROR_LOGIN\")\n",
    "                continue\n",
    "            try:\n",
    "                (d_from, d_to), rows, ck_prev, max_ut = run_customer_for_tenant(client, token, tnt)\n",
    "                if d_from and d_to:\n",
    "                    win_min = d_from if (win_min is None or d_from < win_min) else win_min\n",
    "                    win_max = d_to   if (win_max is None or d_to   > win_max) else win_max\n",
    "                staged_stats[tnt] = {\"rows\": rows, \"max_ut\": max_ut}\n",
    "            except Exception:\n",
    "                logger.exception(f\"[{tnt}] customer error\")\n",
    "\n",
    "        # Chá»‰ MERGE khi cÃ³ cá»­a sá»• áº£nh hÆ°á»Ÿng\n",
    "        if win_min and win_max:\n",
    "            try:\n",
    "                merge_customer_window(client, win_min, win_max)\n",
    "\n",
    "                # MERGE OK â†’ cáº­p nháº­t CK theo tá»«ng tenant vÃ  ghi log MERGED\n",
    "                for tnt, st in staged_stats.items():\n",
    "                    max_ut = st.get(\"max_ut\")\n",
    "                    if max_ut is not None:\n",
    "                        set_ck(\"customer\", tnt, max_ut)\n",
    "                        add_log(tnt, \"customer\", st.get(\"rows\", 0), max_ut, \"MERGED\")\n",
    "                        logger.info(f\"[{tnt}][customer] MERGED window [{win_min}..{win_max}] â†’ set CK = {ms_to_iso(max_ut)}\")\n",
    "\n",
    "            except Exception:\n",
    "                logger.exception(f\"MERGE customer window [{win_min}..{win_max}] failed (CK not advanced)\")\n",
    "\n",
    "        next_customer = loop_start + timedelta(minutes=INTERVAL_CUSTOMER_MIN)\n",
    "    # --- CALL (15')\n",
    "    if loop_start >= next_call:\n",
    "        logger.info(f\"â–¶ Run call_log (all tenants) | interval={INTERVAL_CALL_MIN}m\")\n",
    "        for acc in ACCOUNTS:\n",
    "            tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "            token = get_token(tnt, email, pwd)\n",
    "            if not token:\n",
    "                add_log(tnt, \"call_log\", 0, get_ck(\"call_log\", tnt), \"ERROR_LOGIN\")\n",
    "                continue\n",
    "            try:\n",
    "                run_call_for_tenant(client, token, tnt)\n",
    "            except Exception:\n",
    "                logger.exception(f\"[{tnt}] call_log error\")\n",
    "        next_call = loop_start + timedelta(minutes=INTERVAL_CALL_MIN)\n",
    "\n",
    "    # --- STAFF/GROUP (daily)\n",
    "    if loop_start >= next_staffgrp:\n",
    "        logger.info(\"â–¶ Daily snapshot: staff + group (all tenants)\")\n",
    "        snapshot_staff_group(client)\n",
    "        next_staffgrp = next_daily(loop_start + timedelta(seconds=1), RUN_DAILY_HOUR)\n",
    "\n",
    "\n",
    "    # --- RANK (daily â†’ weekly)\n",
    "    if loop_start >= next_rank:\n",
    "        try:\n",
    "            weekly_key = iso_week_key(loop_start)\n",
    "            logger.info(f\"â–¶ Daily rank_mapping â†’ writing weekly table key={weekly_key}\")\n",
    "            rows = run_rank_mapping_weekly(client, weekly_key)\n",
    "            add_log(\"ALL\", \"rank_mapping\", rows, None, \"REPLACE_PARTITION\")\n",
    "            logger.info(f\"[rank_mapping] REPLACE_PARTITION week={weekly_key} rows={rows} â†’ table=rank_mapping\")\n",
    "        except Exception:\n",
    "            logger.exception(\"rank_mapping error\")\n",
    "        next_rank = next_daily(loop_start + timedelta(seconds=1), RUN_RANK_DAILY_HOUR)\n",
    "\n",
    "    # cuá»‘i tick: flush log\n",
    "    flush_logs(client)\n",
    "    return next_customer, next_call, next_staffgrp, next_rank\n",
    "\n",
    "\n",
    "def plan_initial_windows(now: datetime, staff_last, group_last, rank_last):\n",
    "    # giá»‘ng logic cÅ©\n",
    "    next_customer = now\n",
    "    next_call     = now\n",
    "    if (staff_last is None or (now - staff_last > timedelta(days=1))) \\\n",
    "       or (group_last is None or (now - group_last > timedelta(days=1))):\n",
    "        next_staffgrp = now\n",
    "    else:\n",
    "        next_staffgrp = next_daily(now, RUN_DAILY_HOUR)\n",
    "    if (rank_last is None) or (now - rank_last > timedelta(days=1)):\n",
    "        next_rank = now\n",
    "    else:\n",
    "        next_rank = next_daily(now, RUN_RANK_DAILY_HOUR)\n",
    "    return next_customer, next_call, next_staffgrp, next_rank\n",
    "\n",
    "\n",
    "def run_forever():\n",
    "    client = bootstrap()\n",
    "\n",
    "    # khá»Ÿi táº¡o lá»‹ch láº§n Ä‘áº§u (giá»¯ nguyÃªn)\n",
    "    now = datetime.now(timezone.utc)\n",
    "    staff_last = get_last_run_any(\"staff\")\n",
    "    group_last = get_last_run_any(\"group\")\n",
    "    rank_last  = get_last_run_any(\"rank_mapping\")\n",
    "    next_customer, next_call, next_staffgrp, next_rank = plan_initial_windows(now, staff_last, group_last, rank_last)\n",
    "    logger.info(f\"â±ï¸ Schedule boot | customer/call: now | staff/group: {next_staffgrp} | rank: {next_rank}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            next_customer, next_call, next_staffgrp, next_rank = run_tick(\n",
    "                client, next_customer, next_call, next_staffgrp, next_rank\n",
    "            )\n",
    "            # sleep nhÆ° cÅ©\n",
    "            next_due = min(next_customer, next_call, next_staffgrp, next_rank)\n",
    "            wait_sec = int((next_due - datetime.now(timezone.utc)).total_seconds())\n",
    "            wait_sec = max(1, min(300, wait_sec))\n",
    "            logger.info(f\"â³ Idle {wait_sec}s â€” next due @ {next_due.isoformat()} UTC\")\n",
    "            time.sleep(wait_sec)\n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\"â›” Stopped by user\"); break\n",
    "        except Exception:\n",
    "            logger.exception(\"Loop-level error; continue in 10s\")\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "def run_once(job: str = \"all\"):\n",
    "    \"\"\"\n",
    "    DÃ¹ng cho cron: cháº¡y 1 phÃ¡t rá»“i thoÃ¡t.\n",
    "    job: all | customer | call | staffgroup | rank\n",
    "    \"\"\"\n",
    "    client = bootstrap()\n",
    "    now = datetime.now(timezone.utc)\n",
    "\n",
    "    # thiáº¿t láº­p má»‘c â€œÄ‘áº¿n háº¡n ngay bÃ¢y giá»â€ cho job cáº§n cháº¡y\n",
    "    staff_last = get_last_run_any(\"staff\")\n",
    "    group_last = get_last_run_any(\"group\")\n",
    "    rank_last  = get_last_run_any(\"rank_mapping\")\n",
    "    next_customer, next_call, next_staffgrp, next_rank = plan_initial_windows(now, staff_last, group_last, rank_last)\n",
    "\n",
    "    # Ã©p chá»‰ cháº¡y pháº§n chá»n (Ä‘áº·t cÃ¡c má»‘c khÃ¡c á»Ÿ tÆ°Æ¡ng lai Ä‘á»ƒ bá» qua)\n",
    "    far_future = now + timedelta(days=365*10)\n",
    "    if job != \"all\":\n",
    "        if job != \"customer\":  next_customer  = far_future\n",
    "        if job != \"call\":      next_call      = far_future\n",
    "        if job != \"staffgroup\":next_staffgrp  = far_future\n",
    "        if job != \"rank\":      next_rank      = far_future\n",
    "\n",
    "    # cháº¡y Ä‘Ãºng 1 tick\n",
    "    run_tick(client, next_customer, next_call, next_staffgrp, next_rank)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", choices=[\"daemon\",\"once\"], default=\"once\",\n",
    "                        help=\"daemon = vÃ²ng láº·p vÃ´ háº¡n; once = cháº¡y 1 phÃ¡t cho cron\")\n",
    "    parser.add_argument(\"--job\", choices=[\"all\",\"customer\",\"call\",\"staffgroup\",\"rank\"],\n",
    "                        default=\"all\", help=\"Chá»‰ Ä‘á»‹nh job khi --mode=once\")\n",
    " #  args = parser.parse_args()\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.mode == \"daemon\":\n",
    "        run_forever()\n",
    "    else:\n",
    "        run_once(args.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d896ddf-c3b8-4a3e-86ec-6b56f260c649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 14:19:45+0700 | INFO     | ðŸ”¥ Warmed checkpoint cache: 21 keys (with last_run)\n",
      "2025-09-12 14:19:45+0700 | INFO     | â–¶ [cron] Run customer (all tenants)\n",
      "2025-09-12 14:19:46+0700 | INFO     | [hot1new] token obtained\n",
      "2025-09-12 14:19:46+0700 | INFO     | [hot1new][customer] ck=1757653223812 (2025-09-12T05:00:23.812000+00:00) overlap=180000 â†’ cutoff=2025-09-12T04:57:23.812000+00:00\n",
      "2025-09-12 14:19:47+0700 | INFO     | [hot1new][customer] page=1 got=500 cum=500 last_ts=2025-09-12T07:00:54.359000+00:00 window=[2025-09-12T04:57:23.812000+00:00 â†’ 2025-09-12T07:19:46.664000+00:00] time_coverageâ‰ˆ13.3% totalDocs=446679 hasNext=True\n",
      "2025-09-12 14:19:48+0700 | INFO     | [hot1new][customer] page=2 got=500 cum=1000 last_ts=2025-09-12T06:37:39.062000+00:00 window=[2025-09-12T04:57:23.812000+00:00 â†’ 2025-09-12T07:19:46.664000+00:00] time_coverageâ‰ˆ29.6% totalDocs=446679 hasNext=True\n",
      "2025-09-12 14:19:49+0700 | INFO     | [hot1new][customer] page=3 got=90 cum=1090 last_ts=2025-09-12T04:57:40.611000+00:00 window=[2025-09-12T04:57:23.812000+00:00 â†’ 2025-09-12T07:19:46.664000+00:00] time_coverageâ‰ˆ99.8% totalDocs=446679 hasNext=False\n",
      "2025-09-12 14:19:49+0700 | INFO     | [hot1new][customer] DONE pages=3 loaded=1090 range=[2025-09-12T04:57:23.812000+00:00 â†’ 2025-09-12T07:19:45.625000+00:00]\n",
      "2025-09-12 14:19:54+0700 | INFO     | [hot1new][customer] STAGED rows=1090 window=[2025-09-12..2025-09-12] (ck unchanged)\n",
      "2025-09-12 14:19:54+0700 | INFO     | [hot1old] token obtained\n",
      "2025-09-12 14:19:54+0700 | INFO     | [hot1old][customer] ck=1757653212839 (2025-09-12T05:00:12.839000+00:00) overlap=180000 â†’ cutoff=2025-09-12T04:57:12.839000+00:00\n",
      "2025-09-12 14:19:55+0700 | INFO     | [hot1old][customer] page=1 got=500 cum=500 last_ts=2025-09-12T06:55:46.544000+00:00 window=[2025-09-12T04:57:12.839000+00:00 â†’ 2025-09-12T07:19:54.878000+00:00] time_coverageâ‰ˆ16.9% totalDocs=166358 hasNext=True\n",
      "2025-09-12 14:19:56+0700 | INFO     | [hot1old][customer] page=2 got=354 cum=854 last_ts=2025-09-12T04:57:28.026000+00:00 window=[2025-09-12T04:57:12.839000+00:00 â†’ 2025-09-12T07:19:54.878000+00:00] time_coverageâ‰ˆ99.8% totalDocs=166358 hasNext=False\n",
      "2025-09-12 14:19:56+0700 | INFO     | [hot1old][customer] DONE pages=2 loaded=854 range=[2025-09-12T04:57:12.839000+00:00 â†’ 2025-09-12T07:19:51.752000+00:00]\n",
      "2025-09-12 14:19:59+0700 | INFO     | [hot1old][customer] STAGED rows=854 window=[2025-09-12..2025-09-12] (ck unchanged)\n",
      "2025-09-12 14:20:00+0700 | INFO     | [hot2] token obtained\n",
      "2025-09-12 14:20:00+0700 | INFO     | [hot2][customer] ck=1757653250461 (2025-09-12T05:00:50.461000+00:00) overlap=180000 â†’ cutoff=2025-09-12T04:57:50.461000+00:00\n",
      "2025-09-12 14:20:01+0700 | INFO     | [hot2][customer] page=1 got=500 cum=500 last_ts=2025-09-12T06:58:05.963000+00:00 window=[2025-09-12T04:57:50.461000+00:00 â†’ 2025-09-12T07:20:00.597000+00:00] time_coverageâ‰ˆ15.4% totalDocs=92400 hasNext=True\n",
      "2025-09-12 14:20:02+0700 | INFO     | [hot2][customer] page=2 got=500 cum=1000 last_ts=2025-09-12T05:00:55.342000+00:00 window=[2025-09-12T04:57:50.461000+00:00 â†’ 2025-09-12T07:20:00.597000+00:00] time_coverageâ‰ˆ97.8% totalDocs=92400 hasNext=True\n",
      "2025-09-12 14:20:03+0700 | INFO     | [hot2][customer] page=3 got=17 cum=1017 last_ts=2025-09-12T04:58:06.305000+00:00 window=[2025-09-12T04:57:50.461000+00:00 â†’ 2025-09-12T07:20:00.597000+00:00] time_coverageâ‰ˆ99.8% totalDocs=92400 hasNext=False\n",
      "2025-09-12 14:20:03+0700 | INFO     | [hot2][customer] DONE pages=3 loaded=1017 range=[2025-09-12T04:57:50.461000+00:00 â†’ 2025-09-12T07:20:00.752000+00:00]\n",
      "2025-09-12 14:20:06+0700 | INFO     | [hot2][customer] STAGED rows=1017 window=[2025-09-12..2025-09-12] (ck unchanged)\n",
      "2025-09-12 14:20:16+0700 | INFO     | ðŸ§© MERGE customer done for window [2025-09-12..2025-09-12] & cleaned staging.\n",
      "2025-09-12 14:20:16+0700 | INFO     | [hot1new][customer] MERGED window [2025-09-12..2025-09-12] â†’ CK = 2025-09-12T07:19:45.625000+00:00\n",
      "2025-09-12 14:20:16+0700 | INFO     | [hot1old][customer] MERGED window [2025-09-12..2025-09-12] â†’ CK = 2025-09-12T07:19:51.752000+00:00\n",
      "2025-09-12 14:20:16+0700 | INFO     | [hot2][customer] MERGED window [2025-09-12..2025-09-12] â†’ CK = 2025-09-12T07:20:00.752000+00:00\n",
      "2025-09-12 14:20:19+0700 | INFO     | ðŸ“ Flushed update_log: 6 rows\n",
      "2025-09-12 14:20:19+0700 | INFO     | â–¶ [cron] Run call_log (all tenants)\n",
      "2025-09-12 14:20:19+0700 | INFO     | [hot1new][call_log] ck=1757653261902 (2025-09-12T05:01:01.902000+00:00) â†’ cutoff=1757653261902 (2025-09-12T05:01:01.902000+00:00); window_end=now=2025-09-12T07:20:19.314000+00:00\n",
      "2025-09-12 14:20:20+0700 | INFO     | [hot1new][call_log] page=1 got=500 cum=500 last_ts=2025-09-12T07:00:01.687000+00:00 window=[2025-09-12T05:01:01.902000+00:00 â†’ 2025-09-12T07:20:19.317000+00:00] time_coverageâ‰ˆ14.6% totalDocs=214528 hasNext=True\n",
      "2025-09-12 14:20:21+0700 | INFO     | [hot1new][call_log] page=2 got=496 cum=996 last_ts=2025-09-12T05:01:24.234000+00:00 window=[2025-09-12T05:01:01.902000+00:00 â†’ 2025-09-12T07:20:19.317000+00:00] time_coverageâ‰ˆ99.7% totalDocs=214528 hasNext=False\n",
      "2025-09-12 14:20:21+0700 | INFO     | [hot1new][call_log] DONE pages=2 loaded=996 range=[2025-09-12T05:01:01.902000+00:00 â†’ 2025-09-12T07:20:13.079000+00:00]\n",
      "2025-09-12 14:20:25+0700 | INFO     | [hot1new][call_log] APPEND loaded=996 new_ck=1757661621267 (2025-09-12T07:20:21.267000+00:00)\n",
      "2025-09-12 14:20:25+0700 | INFO     | [hot1old][call_log] ck=1757653202387 (2025-09-12T05:00:02.387000+00:00) â†’ cutoff=1757653202387 (2025-09-12T05:00:02.387000+00:00); window_end=now=2025-09-12T07:20:25.293000+00:00\n",
      "2025-09-12 14:20:26+0700 | INFO     | [hot1old][call_log] page=1 got=500 cum=500 last_ts=2025-09-12T06:52:01.261000+00:00 window=[2025-09-12T05:00:02.387000+00:00 â†’ 2025-09-12T07:20:25.294000+00:00] time_coverageâ‰ˆ20.2% totalDocs=152798 hasNext=True\n",
      "2025-09-12 14:20:26+0700 | INFO     | [hot1old][call_log] page=2 got=197 cum=697 last_ts=2025-09-12T06:34:02.515000+00:00 window=[2025-09-12T05:00:02.387000+00:00 â†’ 2025-09-12T07:20:25.294000+00:00] time_coverageâ‰ˆ33.0% totalDocs=152798 hasNext=False\n",
      "2025-09-12 14:20:26+0700 | INFO     | [hot1old][call_log] DONE pages=2 loaded=697 range=[2025-09-12T05:00:02.387000+00:00 â†’ 2025-09-12T07:20:28.084000+00:00]\n",
      "2025-09-12 14:20:30+0700 | INFO     | [hot1old][call_log] APPEND loaded=697 new_ck=1757661628084 (2025-09-12T07:20:28.084000+00:00)\n",
      "2025-09-12 14:20:30+0700 | INFO     | [hot2][call_log] ck=1757653260103 (2025-09-12T05:01:00.103000+00:00) â†’ cutoff=1757653260103 (2025-09-12T05:01:00.103000+00:00); window_end=now=2025-09-12T07:20:30.095000+00:00\n",
      "2025-09-12 14:20:30+0700 | INFO     | [hot2][call_log] page=1 got=500 cum=500 last_ts=2025-09-12T06:38:34.470000+00:00 window=[2025-09-12T05:01:00.103000+00:00 â†’ 2025-09-12T07:20:30.098000+00:00] time_coverageâ‰ˆ30.1% totalDocs=168096 hasNext=True\n",
      "2025-09-12 14:20:31+0700 | INFO     | [hot2][call_log] page=2 got=178 cum=678 last_ts=2025-09-12T05:04:59.720000+00:00 window=[2025-09-12T05:01:00.103000+00:00 â†’ 2025-09-12T07:20:30.098000+00:00] time_coverageâ‰ˆ97.1% totalDocs=168096 hasNext=False\n",
      "2025-09-12 14:20:31+0700 | INFO     | [hot2][call_log] DONE pages=2 loaded=678 range=[2025-09-12T05:01:00.103000+00:00 â†’ 2025-09-12T07:20:22.988000+00:00]\n",
      "2025-09-12 14:20:34+0700 | INFO     | [hot2][call_log] APPEND loaded=678 new_ck=1757661623882 (2025-09-12T07:20:23.882000+00:00)\n",
      "2025-09-12 14:20:36+0700 | INFO     | ðŸ“ Flushed update_log: 3 rows\n",
      "2025-09-12 14:20:36+0700 | INFO     | â–¶ [cron] Run staff/group snapshot\n",
      "2025-09-12 14:20:36+0700 | INFO     | â–¶ Snapshot staff (MERGE by _id) + group (TRUNCATE)\n",
      "2025-09-12 14:20:37+0700 | INFO     | [hot1new] fetched 64 staff\n",
      "2025-09-12 14:20:39+0700 | INFO     | [hot1old] fetched 87 staff\n",
      "2025-09-12 14:20:41+0700 | INFO     | [hot2] fetched 66 staff\n",
      "2025-09-12 14:20:44+0700 | INFO     | [staff] MERGE in-memory done, input_rows=217\n",
      "2025-09-12 14:20:49+0700 | INFO     | [staff/group] staff_merged=217, group_truncated=5\n",
      "2025-09-12 14:20:51+0700 | INFO     | ðŸ“ Flushed update_log: 2 rows\n",
      "2025-09-12 14:20:51+0700 | INFO     | â–¶ [cron] Run rank_mapping snapshot\n",
      "2025-09-12 14:20:58+0700 | INFO     | [rank_mapping] APPEND rows=176 for week=2025w37 (week_start=2025-09-08)\n",
      "2025-09-12 14:20:58+0700 | INFO     | [rank_mapping] REPLACE_PARTITION week=2025w37 rows=176\n",
      "2025-09-12 14:21:00+0700 | INFO     | ðŸ“ Flushed update_log: 1 rows\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  ETL Callio -> BigQuery (Notebook Runner)\n",
    "#  - Warm checkpoint cache (update_log) 1 láº§n\n",
    "#  - Loop vÃ´ háº¡n, tá»± lÃªn lá»‹ch theo káº¿ hoáº¡ch:\n",
    "#       + customer: 15 phÃºt (delta theo updateTime, DESC + early-stop, overlap)\n",
    "#       + call_log: 15 phÃºt (delta theo createTime, DESC + early-stop, overlap)\n",
    "#       + staff/group: 1 láº§n/ngÃ y (snapshot WRITE_TRUNCATE)\n",
    "#       + rank_mapping: 1 láº§n/tuáº§n (snapshot WRITE_TRUNCATE)  â†’ placeholder\n",
    "#  - Buffer update_log, flush 1 phÃ¡t má»—i vÃ²ng\n",
    "#  - Giá»¯ schema y nhÆ° áº£nh (cÃ³ bÆ°á»›c táº¡o báº£ng náº¿u thiáº¿u)\n",
    "# =========================================\n",
    "\n",
    "import os, sys, json, time, logging, hashlib, math\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound, Conflict, BadRequest\n",
    "\n",
    "# ---------- 1) CREDENTIALS ----------\n",
    "# DÃN Service Account JSON vÃ o Ä‘Ã¢y (hoáº·c set env SERVICE_ACCOUNT_KEY_JSON)\n",
    "SERVICE_ACCOUNT_KEY_JSON = os.getenv(\"SERVICE_ACCOUNT_KEY_JSON\") or r\"\"\"{\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"rio-system-migration\",\n",
    "    \"private_key_id\": \"30a7b13765330a0a061b667aa13f03b72cfe344e\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCmrqE5UEBPBbSv\\ndiOy+nnckHlLHww8Icx+Fma+GxWWaMnhAviX4v/7ED3b9UTwfB8qWYIf0EmETuGA\\nMNXfD1Zk5afZLNg+KfdXCxALZ71+Rd1oxa9fpOTJWD30SvK2GwZk2T/1KvAlsefm\\nlN9EBX2PQ+YteZUh5n2HTcFTZjlrrHVfoRGWDzQdLOX+ZG8JSROybuhZ1ZC0Ebrg\\nCcsry2o+RrQYigaNYgs8EdP/O1iD/jCX+DYigCjIxNqVGg0SIRUJoXJOao6LzUhP\\n0BNuji7GzkFQ2vzXFc50FNrmE5lqTCSAfSSuyItSWux/KMhgTP+hJVh5rACGmUKL\\njPy7q4A7AgMBAAECggEAAR7OXZzA+eb/amiAX+0YEZf1AMDCK8tMXRKYeTGkaQDm\\nTnlfI2I2t0DKMabJ4lXrPbUhT0ZuoBGJqXtqjJfFTGNQyABpqa1NiMu9vV3io60j\\nilc90QZCNWo+7FvumrO+zMG7ENJLj+1MsjuQ7puc1vAtZWPaUUKD2Ht4z68xtpwM\\n2nPXsNhhJKq2rXMKfERk6cDl7dILskJWaHuTs4rYu65eVtfBRDgTlULLQ0ONxm/W\\n6apECFuM8K1R3TwX5XDUEGokyDsAz77ukRAanAEXK8XtADmX7VFK5X7hUtG1+b8p\\nntVAbx2dAZ5W+PHjv81LEXRbqEcC1JJcD4PkH0plNQKBgQDg6qnIvpb2iR79+6Kh\\nU0Do7POsqCgtN0Etex/EDjM71xo21gdZi7g2BJ0RAsblFxqdPsRaHDZZXJcs2R8G\\nqlm+6zs04c8oz/lU6PxTaBrSHBixMdhUvk/YGrudN7YAN4/RLt/loZzP3/BAgco9\\nlLaPckHJNFVETuKO3CgfXaJPnQKBgQC9t69totWF59MCW9Z4+TnAVW14TEetHhD2\\nsC7Ju+6dGPLxXlnuZuBJQIkWo6T0cqJIYF0BumYdKltGjA9KFyhjDbcydJPRZM2q\\nwo0KD1entyXblYMBEMzP0zsgN3+3Jwg+ZlHUxtK+gjKJiTfISWI1ogBDD3vd4G36\\n8yB/sgDDtwKBgDqKmbqYcO6mbhypfIEFDGYUFrCf7CUotpxB6di74XX33Ojc+HjE\\nNyRIOyGMWXyTcOfwyGaz5SmJQgf4U20GtelNjNGM3MDAsSL6qYKEHEcH7R1h3e7g\\nwiN7gc3ADG0uCQ7nZnt8fzZUEVKY9azlokbf9GOMbY0kAzAv+XmAg5i5AoGAT525\\nWio+r05FaDUAQY5dpRB0u0pPvh/jAJOZXwGmNnlU4uQ0m27C6xrRLRYJ0KgW4IbI\\nIUSHO/Adk/KNLAuh4EfOPLddnT9PbDzvEWy03WZn1cndy2GwgfrkUjXYPBV+SSmJ\\nZ+D0agybhsp2BXB+bYGJ2Jqz4b4giXLkjZI27esCgYAQ3iMcyjM3J8UcTb+AMYAW\\nnAFFRcRvvmVtBSlSzCt+K3d9TZpS1fx9+cBi7o842nIGXvg3lppR4N5F7YrrR25I\\ne5zFrYWTfm+beMyvUYmdVjhoO6PclZWXZMykqVMFichTSS8yLXP08y5s7XJUGc18\\n0Kpii9feusS8ogWahhwwQw==\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"rio-bigquery@rio-system-migration.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"116167675015485282559\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/rio-bigquery%40rio-system-migration.iam.gserviceaccount.com\"\n",
    "}\"\"\"\n",
    "\n",
    "# DÃN danh sÃ¡ch tenant Callio (hoáº·c set env CALLIO_ACCOUNTS_JSON)\n",
    "# Má»—i pháº§n tá»­: {\"tenant\":\"hot1new\",\"email\":\"...\",\"password\":\"...\"}\n",
    "ACCOUNTS = json.loads(os.getenv(\"CALLIO_ACCOUNTS_JSON\", \"[]\")) or [\n",
    "    {\"tenant\":\"hot1new\",\"email\":\"hot1new@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "    {\"tenant\":\"hot1old\",\"email\":\"hot1old@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "    {\"tenant\":\"hot2\",   \"email\":\"hot2@gmail.com\",\"password\":\"Huyhoang@123\"},\n",
    "]\n",
    "# ---------- Rank Mapping Config ----------\n",
    "import re, unicodedata, gspread\n",
    "from google.oauth2 import service_account as gsa\n",
    "\n",
    "RANK_SHEET_ID   = os.getenv(\"RANK_SHEET_ID\", \"1U8B9tglIj21GRbC-TJfB549hx7xE-kP6X99ls-Ihsho\")\n",
    "RANK_TAB_NAME   = os.getenv(\"RANK_TAB_NAME\", \"Xáº¾P Háº NG HIá»†N Táº I\")\n",
    "RANK_RANGES_A1 = os.getenv(\"RANK_RANGES_A1\", \"A1:Q\").split(\";\")\n",
    "# ---------- 2) CONFIG ----------\n",
    "CALLIO_API_BASE_URL = os.getenv(\"CALLIO_API_BASE_URL\", \"https://clientapi.phonenet.io\")\n",
    "API_TIMEOUT   = int(os.getenv(\"API_TIMEOUT\", \"90\"))\n",
    "API_PAGE_SIZE = int(os.getenv(\"API_PAGE_SIZE\", \"500\"))\n",
    "\n",
    "BQ_PROJECT_ID = os.getenv(\"BQ_PROJECT_ID\", \"rio-system-migration\")\n",
    "BQ_DATASET_ID = os.getenv(\"BQ_DATASET_ID\", \"dev_callio\")\n",
    "BQ_LOCATION   = os.getenv(\"BQ_LOCATION\", \"asia-southeast1\")\n",
    "\n",
    "# Lá»‹ch cháº¡y\n",
    "INTERVAL_CUSTOMER_MIN = 15\n",
    "INTERVAL_CALL_MIN     = 15\n",
    "RUN_DAILY_HOUR        = 9  # staff/group má»—i ngÃ y\n",
    "RUN_RANK_DAILY_HOUR   = 9  # rank_mapping má»—i ngÃ y\n",
    "\n",
    "# Cá»­a sá»• vÃ  checkpoint\n",
    "OVERLAP_MS = int(os.getenv(\"OVERLAP_MS\", str(3 * 60 * 1000)))      # 3 phÃºt an toÃ n\n",
    "DAYS_TO_FETCH_IF_EMPTY = int(os.getenv(\"DAYS_TO_FETCH_IF_EMPTY\", \"30\"))\n",
    "\n",
    "# Giá»›i háº¡n Ä‘á»ƒ test (None = khÃ´ng giá»›i háº¡n)\n",
    "LIMIT_RECORDS_PER_ENDPOINT: Optional[int] = None\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S%z\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(\"runner\")\n",
    "\n",
    "# ---------- 3) BQ CLIENT ----------\n",
    "def get_bq_client() -> bigquery.Client:\n",
    "    info = json.loads(SERVICE_ACCOUNT_KEY_JSON)\n",
    "    project = info.get(\"project_id\") or BQ_PROJECT_ID\n",
    "    return bigquery.Client.from_service_account_info(info, project=project)\n",
    "\n",
    "def fqn(client: bigquery.Client, table: str) -> str:\n",
    "    return f\"{client.project}.{BQ_DATASET_ID}.{table}\"\n",
    "\n",
    "def ensure_dataset(client: bigquery.Client):\n",
    "    ds = bigquery.Dataset(f\"{client.project}.{BQ_DATASET_ID}\")\n",
    "    ds.location = BQ_LOCATION\n",
    "    try:\n",
    "        client.create_dataset(ds)\n",
    "        logger.info(f\"âœ… Created dataset {BQ_DATASET_ID}\")\n",
    "    except Conflict:\n",
    "        pass\n",
    "\n",
    "# ---------- 4) SCHEMAS (THEO áº¢NH) ----------\n",
    "# Táº¡o báº£ng náº¿u thiáº¿u, cÃ²n náº¿u Ä‘Ã£ cÃ³ -> giá»¯ nguyÃªn\n",
    "def ensure_table_schema_call_log(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"call_log\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"chargeTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"direction\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromNumber\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"toNumber\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"startTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"endTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"duration\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"billDuration\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"hangupCause\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"answerTime\",\"FLOAT64\"),\n",
    "            bigquery.SchemaField(\"fromUser__id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromUser__name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"fromGroup__id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            # ðŸ‘‰ ThÃªm dÃ²ng nÃ y:\n",
    "            bigquery.SchemaField(\"NgayTao\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\"]\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"NgayTao\",\n",
    "        )\n",
    "        client.create_table(t)\n",
    "\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "\n",
    "def ensure_table_schema_staff(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"staff\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"email\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"name\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_group(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"group\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"group_id\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_rank_single(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"rank_mapping\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"code\",         \"STRING\"),\n",
    "            bigquery.SchemaField(\"grade\",        \"STRING\"),\n",
    "            bigquery.SchemaField(\"target_day\",   \"INT64\"),\n",
    "            bigquery.SchemaField(\"target_week\",  \"INT64\"),\n",
    "            bigquery.SchemaField(\"target_month\", \"INT64\"),\n",
    "            bigquery.SchemaField(\"week_key\",     \"STRING\"),\n",
    "            bigquery.SchemaField(\"week_start\",   \"DATE\"),\n",
    "            bigquery.SchemaField(\"snapshot_at\",  \"TIMESTAMP\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY, field=\"week_start\",\n",
    "        )\n",
    "        t.clustering_fields = [\"code\"]\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_customer_main(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"customer\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"assignedTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"phone\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"customField_0_val\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"NgayUpdate\",\"DATE\"),  # â† partition\n",
    "            bigquery.SchemaField(\"NgayAssign\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.clustering_fields = [\"tenant\",\"_id\"]\n",
    "        t.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"NgayUpdate\",\n",
    "        )\n",
    "        # KHÃ”NG báº­t require_partition_filter Ä‘á»ƒ MERGE/APPEND linh hoáº¡t\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def ensure_table_schema_customer_stg(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"stg_customer\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"assignedTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"createTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"phone\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_name\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"user_group_id\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"row_hash\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"customField_0_val\",\"STRING\"),\n",
    "            bigquery.SchemaField(\"NgayUpdate\",\"DATE\"),\n",
    "            bigquery.SchemaField(\"NgayAssign\",\"DATE\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        # staging khÃ´ng cáº§n partition\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "\n",
    "# ---------- 5) UPDATE LOG ----------\n",
    "def ensure_update_log(client: bigquery.Client):\n",
    "    table_id = fqn(client, \"update_log\")\n",
    "    try:\n",
    "        client.get_table(table_id); return\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"table_name\",\"STRING\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"tenant\",\"STRING\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"updated_at\",\"TIMESTAMP\",mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"rows_loaded\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"max_updateTime\",\"INT64\"),\n",
    "            bigquery.SchemaField(\"mode\",\"STRING\"),\n",
    "        ]\n",
    "        t = bigquery.Table(table_id, schema=schema)\n",
    "        t.time_partitioning = bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field=\"updated_at\")\n",
    "        client.create_table(t)\n",
    "        logger.info(f\"âœ… Created table {table_id}\")\n",
    "\n",
    "def sanitize_table_name(name: str) -> str:\n",
    "    s = name.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return s\n",
    "\n",
    "# Cache checkpoint trong RAM\n",
    "CHECKPOINT: Dict[Tuple[str,str], Optional[int]] = {}\n",
    "LAST_RUN: Dict[Tuple[str,str], Optional[datetime]] = {}\n",
    "\n",
    "def warm_checkpoint_cache(client: bigquery.Client):\n",
    "    ensure_update_log(client)\n",
    "    sql = f\"\"\"\n",
    "      SELECT table_name, tenant, \n",
    "             MAX(max_updateTime) AS ck,\n",
    "             MAX(updated_at)     AS last_run_at\n",
    "      FROM `{fqn(client, \"update_log\")}`\n",
    "      GROUP BY table_name, tenant\n",
    "    \"\"\"\n",
    "    for r in client.query(sql, location=BQ_LOCATION).result():\n",
    "        tbl = str(r[\"table_name\"]).lower()\n",
    "        tnt = r[\"tenant\"]\n",
    "        CHECKPOINT[(tbl, tnt)] = int(r[\"ck\"]) if r[\"ck\"] is not None else None\n",
    "        dt = r[\"last_run_at\"]\n",
    "        if isinstance(dt, datetime) and dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        LAST_RUN[(tbl, tnt)] = dt\n",
    "    logger.info(f\"ðŸ”¥ Warmed checkpoint cache: {len(CHECKPOINT)} keys (with last_run)\")\n",
    "\n",
    "def get_last_run_any(table_name: str) -> Optional[datetime]:\n",
    "    \"\"\"Láº¥y láº§n cháº¡y gáº§n nháº¥t cá»§a má»™t báº£ng (gom má»i tenant).\"\"\"\n",
    "    tbl = sanitize_table_name(table_name)\n",
    "    dts = [dt for (t, _), dt in LAST_RUN.items() if t == tbl and dt is not None]\n",
    "    return max(dts) if dts else None\n",
    "\n",
    "def get_last_run_by_prefix(prefix: str) -> Optional[datetime]:\n",
    "    \"\"\"Láº¥y láº§n cháº¡y gáº§n nháº¥t cho cÃ¡c báº£ng cÃ³ tiá»n tá»‘ (vd rank_mapping_YYYYwWW).\"\"\"\n",
    "    pfx = sanitize_table_name(prefix)\n",
    "    dts = [dt for (t, _), dt in LAST_RUN.items() if t.startswith(pfx) and dt is not None]\n",
    "    return max(dts) if dts else None\n",
    "\n",
    "\n",
    "def get_ck(table: str, tenant: str) -> Optional[int]:\n",
    "    return CHECKPOINT.get((sanitize_table_name(table), tenant))\n",
    "\n",
    "def set_ck(table: str, tenant: str, value: Optional[int]):\n",
    "    CHECKPOINT[(sanitize_table_name(table), tenant)] = value\n",
    "\n",
    "# Buffer log, flush 1 phÃ¡t\n",
    "PENDING_LOGS: List[Dict[str, Any]] = []\n",
    "\n",
    "def add_log(tenant: str, table_name: str, rows: int, max_ut_ms: Optional[int], mode: str):\n",
    "    PENDING_LOGS.append({\n",
    "        \"table_name\": sanitize_table_name(table_name),\n",
    "        \"tenant\": tenant,\n",
    "        \"updated_at\": datetime.now(timezone.utc),\n",
    "        \"rows_loaded\": int(rows),\n",
    "        \"max_updateTime\": int(max_ut_ms) if max_ut_ms is not None else None,\n",
    "        \"mode\": mode,\n",
    "    })\n",
    "\n",
    "def flush_logs(client: bigquery.Client):\n",
    "    if not PENDING_LOGS: return\n",
    "    ensure_update_log(client)\n",
    "    df = pd.DataFrame(PENDING_LOGS)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        autodetect=False,\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client,\"update_log\"),\n",
    "                                     job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    logger.info(f\"ðŸ“ Flushed update_log: {len(PENDING_LOGS)} rows\")\n",
    "    PENDING_LOGS.clear()\n",
    "\n",
    "# ---------- 6) UTIL & TRANSFORM ----------\n",
    "def iso_week_key(dt: datetime) -> str:\n",
    "    # Tráº£ vá» dáº¡ng YYYYwWW (vd: 2025w36)\n",
    "    iso = dt.isocalendar()\n",
    "    try:\n",
    "        year, week = iso.year, iso.week\n",
    "    except AttributeError:\n",
    "        # Python cÅ© tráº£ tuple (year, week, weekday)\n",
    "        year, week = iso[0], iso[1]\n",
    "    return f\"{int(year)}w{int(week):02d}\"\n",
    "\n",
    "def week_start_vn(dt: datetime) -> datetime.date:\n",
    "    # TÃ­nh Ä‘áº§u tuáº§n theo VN (Thá»© 2) dÃ¹ng offset +7h cho cháº¯c\n",
    "    local = dt + timedelta(hours=7)\n",
    "    return (local.date() - timedelta(days=(local.isoweekday() - 1)))\n",
    "\n",
    "def safe_eval(v):\n",
    "    if isinstance(v, str):\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            try:\n",
    "                import ast; return ast.literal_eval(v)\n",
    "            except Exception:\n",
    "                return v\n",
    "    return v\n",
    "\n",
    "def derive_cf0_string_from_df(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"customFields\" not in df.columns:\n",
    "        return pd.Series([None]*len(df), dtype=\"string\")\n",
    "\n",
    "    def pick(x):\n",
    "        x = safe_eval(x)\n",
    "        first = None\n",
    "        if isinstance(x, list) and x: first = x[0]\n",
    "        elif isinstance(x, dict):     first = x\n",
    "        if first is None: return None\n",
    "\n",
    "        cand = None\n",
    "        if isinstance(first, dict):\n",
    "            for k in (\"val\",\"value\",\"values\",\"text\",\"name\"):\n",
    "                if k in first: cand = first[k]; break\n",
    "        else:\n",
    "            cand = first\n",
    "\n",
    "        vals = []\n",
    "        if cand is None: return None\n",
    "        if isinstance(cand, list):\n",
    "            for it in cand:\n",
    "                if isinstance(it, dict):\n",
    "                    v = it.get(\"value\") or it.get(\"name\") or it.get(\"text\")\n",
    "                else:\n",
    "                    v = it\n",
    "                if v is None: continue\n",
    "                s = str(v).strip()\n",
    "                if s and s.lower() != \"nan\": vals.append(s)\n",
    "        else:\n",
    "            s = str(cand).strip()\n",
    "            if s and s.lower() != \"nan\": vals.append(s)\n",
    "        if not vals: return None\n",
    "        # de-dup nhÆ°ng giá»¯ thá»© tá»±\n",
    "        seen=set(); out=[]\n",
    "        for v in vals:\n",
    "            if v in seen: continue\n",
    "            seen.add(v); out.append(v)\n",
    "        return \" | \".join(out)\n",
    "\n",
    "    return df[\"customFields\"].apply(pick).astype(\"string\")\n",
    "\n",
    "def extract_user_id(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict): return x.get(\"_id\") or x.get(\"id\")\n",
    "        if isinstance(x, (list,tuple)):\n",
    "            try: d=dict(x); return d.get(\"_id\") or d.get(\"id\")\n",
    "            except Exception: return None\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "\n",
    "def extract_user_name(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict): return x.get(\"name\")\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "\n",
    "def extract_user_group_id(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"user\" not in df.columns: return pd.Series([None]*len(df), dtype=\"string\")\n",
    "    def _get(x):\n",
    "        x = safe_eval(x)\n",
    "        if isinstance(x, dict):\n",
    "            g = x.get(\"group\")\n",
    "            if isinstance(g, dict): return g.get(\"_id\") or g.get(\"id\")\n",
    "            return g\n",
    "        return None\n",
    "    return df[\"user\"].apply(_get).astype(\"string\")\n",
    "    \n",
    "def compute_row_hash(df: pd.DataFrame) -> pd.Series:\n",
    "    if df.empty: return pd.Series([], dtype=\"string\")\n",
    "    volatile = {\"row_hash\",\"updateTime\",\"createTime\",\"updatedAt\",\"createdAt\",\n",
    "                \"NgayTao\",\"NgayUpdate\",\"NgayAssign\"}\n",
    "    cols = [c for c in df.columns if c not in volatile]\n",
    "    def _h(row):\n",
    "        return hashlib.md5(json.dumps({c: row.get(c) for c in cols}, sort_keys=True, default=str).encode(\"utf-8\")).hexdigest()\n",
    "    return df[cols].apply(lambda r: _h(r), axis=1)\n",
    "\n",
    "def yyyymm_from_ms(ms: Optional[int]) -> str:\n",
    "    if ms is None: \n",
    "        return datetime.now(timezone.utc).strftime(\"%Y%m\")\n",
    "    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).strftime(\"%Y%m\")\n",
    "\n",
    "def ms_to_iso(ms: Optional[int]) -> str:\n",
    "    if ms is None: return \"None\"\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(ms)/1000, tz=timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return str(ms)\n",
    "\n",
    "def pct(x: float) -> str:\n",
    "    try:\n",
    "        return f\"{max(0.0, min(1.0, x))*100:.1f}%\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ---------- 7) API ----------\n",
    "TOKENS: Dict[str, Tuple[str, float]] = {}  # tenant -> (token, expire_epoch)\n",
    "\n",
    "def get_token(tenant: str, email: Optional[str], password: Optional[str], force: bool = False) -> Optional[str]:\n",
    "    now = time.time()\n",
    "    tk, exp = TOKENS.get(tenant, (None, 0))\n",
    "    if (not force) and tk and now < exp:\n",
    "        return tk\n",
    "\n",
    "    # fallback email/pwd tá»« ACCOUNTS náº¿u tham sá»‘ None\n",
    "    if not email or not password:\n",
    "        acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "        if acc:\n",
    "            email, password = acc.get(\"email\"), acc.get(\"password\")\n",
    "\n",
    "    if not email or not password:\n",
    "        logger.error(f\"[{tenant}] thiáº¿u email/password Ä‘á»ƒ login Callio\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        new_tk = callio_login(email, password)\n",
    "        TOKENS[tenant] = (new_tk, now + 25*60)  # TTL ~25 phÃºt\n",
    "        logger.info(f\"[{tenant}] token {'refreshed' if force else 'obtained'}\")\n",
    "        return new_tk\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[{tenant}] login error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def callio_login(email: str, password: str) -> str:\n",
    "    r = requests.post(f\"{CALLIO_API_BASE_URL}/auth/login\",\n",
    "                      json={\"email\": email, \"password\": password}, timeout=API_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    tk = (r.json() or {}).get(\"token\")\n",
    "    if not tk: raise RuntimeError(\"Cannot get Callio token\")\n",
    "    return tk\n",
    "\n",
    "def fetch_desc_until(endpoint: str,\n",
    "                     tenant: str, email: str, password: str,\n",
    "                     time_field: str, cutoff_ms: int, log_prefix: str = \"\") -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Láº¥y theo DESC vÃ  dá»«ng khi record cÃ³ time_field <= cutoff_ms.\n",
    "    Tá»± refresh token vÃ  retry 1 láº§n náº¿u gáº·p 401.\n",
    "    Log tiáº¿n Ä‘á»™ tá»«ng trang: tá»•ng gom, last_ts, % phá»§ thá»i gian.\n",
    "    \"\"\"\n",
    "    token = get_token(tenant, email, password)\n",
    "    if not token:\n",
    "        raise RuntimeError(f\"[{tenant}] cannot obtain token\")\n",
    "\n",
    "    headers = {\"token\": token}\n",
    "    page, all_docs = 1, []\n",
    "    window_end_ms = int(time.time() * 1000)\n",
    "    denom = max(1, window_end_ms - int(cutoff_ms or 0))  # trÃ¡nh chia 0\n",
    "\n",
    "    while True:\n",
    "        params = {\"page\": page, \"pageSize\": API_PAGE_SIZE, \"sort\": f\"{time_field}DESC\"}\n",
    "\n",
    "        # --- request + retry 401 má»™t láº§n\n",
    "        try:\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/{endpoint}\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "            if r.status_code == 401:\n",
    "                logger.warning(f\"{log_prefix} 401 on page={page} â†’ refreshing token and retry once\")\n",
    "                token = get_token(tenant, email, password, force=True)\n",
    "                if not token:\n",
    "                    raise RuntimeError(f\"[{tenant}] token refresh failed\")\n",
    "                headers = {\"token\": token}\n",
    "                r = requests.get(f\"{CALLIO_API_BASE_URL}/{endpoint}\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "        except Exception as ex:\n",
    "            raise\n",
    "\n",
    "        r.raise_for_status()\n",
    "        data = r.json() or {}\n",
    "        docs = data.get(\"docs\") or []\n",
    "        total_docs = data.get(\"totalDocs\") or data.get(\"total\") or None\n",
    "        has_next = bool(data.get(\"hasNextPage\", False))\n",
    "        cur_count, stop_flag = 0, False\n",
    "\n",
    "        for d in docs:\n",
    "            ts = int(d.get(time_field) or 0)\n",
    "            if ts <= cutoff_ms:\n",
    "                stop_flag = True\n",
    "                break\n",
    "            all_docs.append(d)\n",
    "            cur_count += 1\n",
    "\n",
    "        last_ts = int(all_docs[-1].get(time_field)) if all_docs else None\n",
    "        progress = (window_end_ms - (last_ts or window_end_ms)) / denom if denom > 0 else 0.0\n",
    "\n",
    "        logger.info(\n",
    "            f\"{log_prefix} page={page} got={cur_count} cum={len(all_docs)} \"\n",
    "            f\"last_ts={ms_to_iso(last_ts)} window=[{ms_to_iso(cutoff_ms)} â†’ {ms_to_iso(window_end_ms)}] \"\n",
    "            f\"time_coverageâ‰ˆ{pct(progress)} totalDocs={total_docs} hasNext={has_next and not stop_flag}\"\n",
    "        )\n",
    "\n",
    "        if LIMIT_RECORDS_PER_ENDPOINT and len(all_docs) >= LIMIT_RECORDS_PER_ENDPOINT:\n",
    "            all_docs = all_docs[:LIMIT_RECORDS_PER_ENDPOINT]\n",
    "            logger.info(f\"{log_prefix} hit LIMIT_RECORDS_PER_ENDPOINT={LIMIT_RECORDS_PER_ENDPOINT}\")\n",
    "            break\n",
    "\n",
    "        if stop_flag or not has_next:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    logger.info(f\"{log_prefix} DONE pages={page} loaded={len(all_docs)} \"\n",
    "                f\"range=[{ms_to_iso(cutoff_ms)} â†’ {ms_to_iso(all_docs[0].get(time_field) if all_docs else None)}]\")\n",
    "    return all_docs\n",
    "\n",
    "# ---------- 8) LOADERS ----------\n",
    "def ensure_unique_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    seen, out = {}, []\n",
    "    for c in df.columns:\n",
    "        if c in seen:\n",
    "            seen[c]+=1; out.append(f\"{c}__{seen[c]}\")\n",
    "        else:\n",
    "            seen[c]=0;  out.append(c)\n",
    "    df.columns = out\n",
    "    return df\n",
    "\n",
    "def load_append(client: bigquery.Client, df: pd.DataFrame, table: str):\n",
    "    if df.empty: return 0\n",
    "    df = ensure_unique_columns(df)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        autodetect=True,\n",
    "        schema_update_options=[\n",
    "            bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION,\n",
    "            bigquery.SchemaUpdateOption.ALLOW_FIELD_RELAXATION\n",
    "        ],\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client, table), job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    return len(df)\n",
    "\n",
    "def load_truncate(client: bigquery.Client, df: pd.DataFrame, table: str):\n",
    "    if df.empty: return 0\n",
    "    df = ensure_unique_columns(df)\n",
    "    job_cfg = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True,\n",
    "    )\n",
    "    client.load_table_from_dataframe(df, fqn(client, table), job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "    return len(df)\n",
    "\n",
    "def _strip_accents_upper(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", str(s or \"\"))\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "    return s.upper().strip()\n",
    "\n",
    "def _extract_code(token: str) -> Optional[str]:\n",
    "    s = _strip_accents_upper(token)\n",
    "    m = re.search(r\"\\b([A-Z]{2}\\d{2})\\b\", s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _clean_grade(token: str) -> Optional[str]:\n",
    "    s = _strip_accents_upper(token)\n",
    "    m = re.search(r\"([A-Z])\", s)\n",
    "    return m.group(1) if m else (s or None)\n",
    "\n",
    "def _df_from_block(rows: List[List[Any]]) -> pd.DataFrame:\n",
    "    # Tráº£ vá»: code, grade, target_day, target_week, target_month\n",
    "    if not rows or len(rows) < 2:\n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\"])\n",
    "\n",
    "    # âœ… Header = dÃ²ng 1\n",
    "    header = rows[0]\n",
    "    body   = rows[1:]\n",
    "\n",
    "    # Chuáº©n hoÃ¡ Ä‘á»™ rá»™ng\n",
    "    width_body = max((len(r) for r in body), default=len(header))\n",
    "    width = max(len(header), width_body)\n",
    "    header = (header + [None]*(width - len(header)))[:width]\n",
    "    body_norm = [(r + [None]*(width - len(r)))[:width] for r in body]\n",
    "\n",
    "    def norm(s): return _strip_accents_upper(s or \"\")\n",
    "    H = [norm(x) for x in header]\n",
    "\n",
    "    # TÃ¬m cá»™t theo tÃªn (Æ°u tiÃªn tiáº¿ng Viá»‡t)\n",
    "    def find_col(words_any=None, must_have=None):\n",
    "        def norm_tok(x): return _strip_accents_upper(x or \"\")\n",
    "        for i, h in enumerate(H):  # H Ä‘Ã£ lÃ  header normalize\n",
    "            ok = True\n",
    "            if must_have:\n",
    "                ok = all(norm_tok(m) in h for m in must_have)\n",
    "            if ok and words_any:\n",
    "                ok = any(norm_tok(w) in h for w in words_any)\n",
    "            if ok: return i\n",
    "        return None\n",
    "\n",
    "    # mapping tÃªn cá»™t\n",
    "    code_idx  = find_col(words_any=(\"MA NV\",\"MA NHAN VIEN\",\"NHAN VIEN\",\"MA\",\"CODE\"))\n",
    "    grade_idx = find_col(words_any=(\"HANG\",\"CAP\",\"BAC\",\"RANK\",\"GRADE\"))\n",
    "    day_idx   = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"NGAY\",))\n",
    "    week_idx  = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"TUAN\",))\n",
    "    month_idx = find_col(words_any=(\"TARGET\",\"CHI TIEU\",\"MUC TIEU\",\"CT\"), must_have=(\"THANG\",))\n",
    "\n",
    "    df = pd.DataFrame(body_norm, columns=range(width))\n",
    "\n",
    "    # fallback theo ná»™i dung náº¿u header khÃ´ng rÃµ\n",
    "    if code_idx is None:\n",
    "        hits = {i: df[i].astype(str).map(_extract_code).notna().sum() for i in range(width)}\n",
    "        code_idx = max(hits, key=hits.get) if hits and max(hits.values())>0 else None\n",
    "    if grade_idx is None:\n",
    "        hits = {i: df[i].astype(str).map(_clean_grade).notna().sum() for i in range(width)}\n",
    "        grade_idx = max(hits, key=hits.get) if hits and max(hits.values())>0 else None\n",
    "\n",
    "    if code_idx is None:\n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\"])\n",
    "\n",
    "    def to_int(v):\n",
    "        s = str(v or \"\").strip()\n",
    "        s = s.replace(\" \", \"\").replace(\",\", \"\").replace(\".\", \"\")\n",
    "        m = re.findall(r\"\\d+\", s)\n",
    "        return int(\"\".join(m)) if m else None\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"code\":         df[code_idx].map(_extract_code),\n",
    "        \"grade\":        df[grade_idx].map(_clean_grade) if grade_idx is not None else None,\n",
    "        \"target_day\":   df[day_idx].map(to_int)   if day_idx   is not None else None,\n",
    "        \"target_week\":  df[week_idx].map(to_int)  if week_idx  is not None else None,\n",
    "        \"target_month\": df[month_idx].map(to_int) if month_idx is not None else None,\n",
    "    })\n",
    "    out = out[out[\"code\"].notna()].drop_duplicates(\"code\", keep=\"first\")\n",
    "    return out\n",
    "\n",
    "def read_rank_mapping() -> pd.DataFrame:\n",
    "    info = json.loads(SERVICE_ACCOUNT_KEY_JSON)\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
    "    creds = gsa.Credentials.from_service_account_info(info, scopes=scopes)\n",
    "    gc = gspread.authorize(creds)\n",
    "    ws = gc.open_by_key(RANK_SHEET_ID).worksheet(RANK_TAB_NAME)\n",
    "    parts = []\n",
    "    for rng in [r.strip() for r in RANK_RANGES_A1 if r.strip()]:\n",
    "        rows = ws.get(rng)\n",
    "        dfp = _df_from_block(rows)\n",
    "        if not dfp.empty:\n",
    "            parts.append(dfp)\n",
    "    if not parts: \n",
    "        return pd.DataFrame(columns=[\"code\",\"grade\"])\n",
    "    all_df = pd.concat(parts, ignore_index=True).drop_duplicates(\"code\", keep=\"first\")\n",
    "    return all_df\n",
    "\n",
    "def run_rank_mapping_weekly(client: bigquery.Client, weekly_key: str):\n",
    "    # 1) Äá»c sheet & chuáº©n hÃ³a\n",
    "    df_map = read_rank_mapping()\n",
    "    if df_map.empty:\n",
    "        logger.warning(\"âš ï¸ rank_mapping trá»‘ng â€“ bá» qua\")\n",
    "        return 0\n",
    "\n",
    "    # 2) Äáº£m báº£o báº£ng Ä‘Ã­ch (schema khÃ´ng cÃ³ grade_value)\n",
    "    ensure_table_schema_rank_single(client)\n",
    "\n",
    "    # 3) Chuáº©n hÃ³a nháº¹ cÃ¡c cá»™t hiá»‡n cÃ³\n",
    "    df_map = df_map.copy()\n",
    "    if \"grade\" in df_map.columns:\n",
    "        df_map[\"grade\"] = df_map[\"grade\"].astype(\"string\").str.upper().str.strip()\n",
    "    if \"code\" in df_map.columns:\n",
    "        df_map[\"code\"] = df_map[\"code\"].astype(\"string\").str.upper().str.strip()\n",
    "\n",
    "    # 4) Gáº¯n key tuáº§n & timestamp\n",
    "    now = datetime.now(timezone.utc)\n",
    "    ws = week_start_vn(now)  # DATE (thá»© 2 VN)\n",
    "    df_map[\"week_key\"]    = weekly_key\n",
    "    df_map[\"week_start\"]  = ws\n",
    "    df_map[\"snapshot_at\"] = pd.Timestamp.utcnow()\n",
    "\n",
    "    # 5) XoÃ¡ dá»¯ liá»‡u tuáº§n nÃ y rá»“i append\n",
    "    full_tbl = fqn(client, \"rank_mapping\")\n",
    "    del_sql = f\"DELETE FROM `{full_tbl}` WHERE week_start = @ws\"\n",
    "    job_cfg = bigquery.QueryJobConfig(\n",
    "        query_parameters=[bigquery.ScalarQueryParameter(\"ws\", \"DATE\", ws)]\n",
    "    )\n",
    "    client.query(del_sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    # 6) Chá»‰ giá»¯ cÃ¡c cá»™t cáº§n thiáº¿t khi load (phÃ²ng khi read_rank_mapping tráº£ nhiá»u cá»™t)\n",
    "    wanted = [c for c in [\"code\",\"grade\",\"target_day\",\"target_week\",\"target_month\",\n",
    "                          \"week_key\",\"week_start\",\"snapshot_at\"] if c in df_map.columns]\n",
    "    rows = load_append(client, df_map[wanted], \"rank_mapping\")\n",
    "    logger.info(f\"[rank_mapping] APPEND rows={rows} for week={weekly_key} (week_start={ws})\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 9) FLOWS ----------\n",
    "def _py(v):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # None / NaN â†’ None\n",
    "    if v is None:\n",
    "        return None\n",
    "    try:\n",
    "        if pd.isna(v):\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # pandas timestamp â†’ ms (náº¿u muá»‘n int64 ms)\n",
    "    if isinstance(v, pd.Timestamp):\n",
    "        return int(v.value // 10**6)\n",
    "\n",
    "    # pandas Int64Dtype scalar â†’ int\n",
    "    try:\n",
    "        if isinstance(v, pd.Int64Dtype().type):\n",
    "            return int(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # numpy scalar â†’ python\n",
    "    if isinstance(v, np.integer):\n",
    "        return int(v)\n",
    "    if isinstance(v, np.floating):\n",
    "        return float(v)\n",
    "    if isinstance(v, np.bool_):\n",
    "        return bool(v)\n",
    "\n",
    "    return v\n",
    "\n",
    "def merge_staff_in_memory(client: bigquery.Client, df_staff: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    MERGE staff theo (tenant, _id) báº±ng ARRAY<STRUCT> param (khÃ´ng staging).\n",
    "    \"\"\"\n",
    "    ensure_table_schema_staff(client)\n",
    "\n",
    "    need = [\"_id\",\"email\",\"name\",\"updateTime\",\"createTime\",\"group_id\",\"tenant\",\"row_hash\"]\n",
    "    for c in need:\n",
    "        if c not in df_staff.columns:\n",
    "            df_staff[c] = None\n",
    "    df = df_staff[need].copy()\n",
    "\n",
    "    # khÃ³a báº¯t buá»™c\n",
    "    df = df[df[\"_id\"].notna() & df[\"tenant\"].notna()]\n",
    "    if df.empty:\n",
    "        logger.info(\"[staff] nothing to merge (empty after key filter)\")\n",
    "        return 0\n",
    "\n",
    "    # Ã©p kiá»ƒu object -> string (hoáº·c None)\n",
    "    for c in [\"_id\",\"email\",\"name\",\"group_id\",\"tenant\",\"row_hash\"]:\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "        df[c] = df[c].where(df[c].notna(), None)\n",
    "\n",
    "    # Ã©p numeric -> Int64 (ms)\n",
    "    for col in (\"updateTime\",\"createTime\"):\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # (tuá»³ chá»n) dedup trÆ°á»›c Ä‘á»ƒ giáº£m payload (SQL cÅ©ng QUALIFY rá»“i)\n",
    "    df = df.sort_values([\"tenant\",\"_id\",\"updateTime\"], ascending=[True, True, False]) \\\n",
    "           .drop_duplicates(subset=[\"tenant\",\"_id\"], keep=\"first\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.info(\"[staff] nothing to merge (empty after dedup)\")\n",
    "        return 0\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    MERGE `{client.project}.{BQ_DATASET_ID}.staff` T\n",
    "    USING (\n",
    "      SELECT *\n",
    "      FROM (\n",
    "        SELECT * FROM UNNEST(@rows)\n",
    "      )\n",
    "      QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY tenant, _id\n",
    "        ORDER BY SAFE_CAST(updateTime AS INT64) DESC\n",
    "      ) = 1\n",
    "    ) S\n",
    "    ON  T.tenant = S.tenant AND T._id = S._id\n",
    "    WHEN MATCHED AND (\n",
    "      (T.row_hash IS NULL OR T.row_hash != S.row_hash)\n",
    "      OR (SAFE_CAST(S.updateTime AS INT64) >= SAFE_CAST(T.updateTime AS INT64) OR T.updateTime IS NULL)\n",
    "    ) THEN UPDATE SET\n",
    "      email      = S.email,\n",
    "      name       = S.name,\n",
    "      updateTime = S.updateTime,\n",
    "      createTime = S.createTime,\n",
    "      group_id   = S.group_id,\n",
    "      row_hash   = S.row_hash\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "      _id, email, name, updateTime, createTime, group_id, tenant, row_hash\n",
    "    ) VALUES (\n",
    "      S._id, S.email, S.name, S.updateTime, S.createTime, S.group_id, S.tenant, S.row_hash\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def make_struct(row) -> bigquery.StructQueryParameter:\n",
    "        return bigquery.StructQueryParameter(\n",
    "            \"\",  # anonymous\n",
    "            bigquery.ScalarQueryParameter(\"_id\",        \"STRING\", _py(row[\"_id\"])),\n",
    "            bigquery.ScalarQueryParameter(\"email\",      \"STRING\", _py(row[\"email\"])),\n",
    "            bigquery.ScalarQueryParameter(\"name\",       \"STRING\", _py(row[\"name\"])),\n",
    "            bigquery.ScalarQueryParameter(\"updateTime\", \"INT64\",  _py(row[\"updateTime\"])),\n",
    "            bigquery.ScalarQueryParameter(\"createTime\", \"INT64\",  _py(row[\"createTime\"])),\n",
    "            bigquery.ScalarQueryParameter(\"group_id\",   \"STRING\", _py(row[\"group_id\"])),\n",
    "            bigquery.ScalarQueryParameter(\"tenant\",     \"STRING\", _py(row[\"tenant\"])),\n",
    "            bigquery.ScalarQueryParameter(\"row_hash\",   \"STRING\", _py(row[\"row_hash\"])),\n",
    "        )\n",
    "\n",
    "    CHUNK = 2000  # háº¡ xuá»‘ng 1000 náº¿u payload to\n",
    "    total = 0\n",
    "    for i in range(0, len(df), CHUNK):\n",
    "        part = df.iloc[i:i+CHUNK]\n",
    "        if part.empty:\n",
    "            continue\n",
    "        structs = [make_struct(r) for _, r in part.iterrows()]\n",
    "        if not structs:\n",
    "            continue\n",
    "        rows_param = bigquery.ArrayQueryParameter(\"rows\", \"STRUCT\", structs)\n",
    "        job_cfg = bigquery.QueryJobConfig(query_parameters=[rows_param])\n",
    "\n",
    "        client.query(sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "        total += len(part)\n",
    "\n",
    "    logger.info(f\"[staff] MERGE in-memory done, input_rows={total}\")\n",
    "    return total\n",
    "    \n",
    "    \n",
    "def run_customer_for_tenant(client: bigquery.Client, token_unused: str, tenant: str):\n",
    "    table_key = \"customer\"\n",
    "    ck = get_ck(table_key, tenant)\n",
    "    if ck is None:\n",
    "        ck = int((datetime.now(timezone.utc) - timedelta(days=DAYS_TO_FETCH_IF_EMPTY)).timestamp()*1000)\n",
    "    cutoff = ck - OVERLAP_MS if ck else ck\n",
    "\n",
    "    logger.info(f\"[{tenant}][customer] ck={ck} ({ms_to_iso(ck)}) overlap={OVERLAP_MS} â†’ cutoff={ms_to_iso(cutoff)}\")\n",
    "\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    docs = fetch_desc_until(\"customer\", tenant, email, password, \"updateTime\", cutoff, log_prefix=f\"[{tenant}][customer]\")\n",
    "    if not docs:\n",
    "        # NOOP: khÃ´ng Ä‘á»•i CK, nhÆ°ng váº«n cÃ³ thá»ƒ muá»‘n nhÃ¬n tháº¥y log\n",
    "        add_log(tenant, table_key, 0, None, \"NOOP\")\n",
    "        return (None, None), 0, ck, None\n",
    "\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    df[\"user_id\"]       = extract_user_id(df)\n",
    "    df[\"user_name\"]     = extract_user_name(df)\n",
    "    df[\"user_group_id\"] = extract_user_group_id(df)\n",
    "\n",
    "    if \"customField_0_val\" not in df.columns:\n",
    "        df[\"customField_0_val\"] = derive_cf0_string_from_df(df)\n",
    "    df[\"customField_0_val\"] = df[\"customField_0_val\"].astype(\"string\")\n",
    "\n",
    "    keep = [\n",
    "        \"_id\",\"assignedTime\",\"createTime\",\"updateTime\",\"name\",\"phone\",\n",
    "        \"user_id\",\"user_name\",\"user_group_id\",\n",
    "        \"tenant\",\"row_hash\",\"customField_0_val\",\n",
    "        \"NgayUpdate\",\"NgayAssign\"\n",
    "    ]\n",
    "    for c in keep:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    out = df[keep].copy()\n",
    "    out[\"tenant\"]      = tenant\n",
    "    out[\"NgayUpdate\"]  = pd.to_datetime(pd.to_numeric(out[\"updateTime\"], errors=\"coerce\"), unit=\"ms\", utc=True).dt.date\n",
    "    out[\"NgayAssign\"]  = pd.to_datetime(pd.to_numeric(out[\"assignedTime\"], errors=\"coerce\"), unit=\"ms\", utc=True).dt.date\n",
    "    out[\"row_hash\"]    = compute_row_hash(out)\n",
    "\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "    rows = load_append(client, out, \"stg_customer\")\n",
    "\n",
    "    # max_ut theo tenant â€“ sáº½ dÃ¹ng Ä‘á»ƒ set CK SAU MERGE\n",
    "    max_ut = int(pd.to_numeric(out[\"updateTime\"], errors=\"coerce\").max()) if rows else None\n",
    "\n",
    "    # Log STAGED nhÆ°ng KHÃ”NG Ä‘Ã­nh kÃ¨m max_updateTime (Ä‘á»ƒ khÃ´ng kÃ©o CK)\n",
    "    d_min = pd.to_datetime(out[\"NgayUpdate\"]).min()\n",
    "    d_max = pd.to_datetime(out[\"NgayUpdate\"]).max()\n",
    "    add_log(tenant, table_key, int(rows), None, \"STAGED\")\n",
    "    logger.info(f\"[{tenant}][customer] STAGED rows={rows} window=[{d_min.date()}..{d_max.date()}] (ck unchanged)\")\n",
    "    return (d_min.date(), d_max.date()), int(rows), ck, max_ut\n",
    "\n",
    "\n",
    "def merge_customer_window(client: bigquery.Client, d_from: datetime.date, d_to: datetime.date):\n",
    "    \"\"\"MERGE stg_customer -> customer, chá»‰ scan cÃ¡c partition trong [d_from..d_to].\"\"\"\n",
    "    ensure_table_schema_customer_main(client)\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "\n",
    "    full_stg = fqn(client, \"stg_customer\")\n",
    "    full_tgt = fqn(client, \"customer\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    DECLARE d_from DATE DEFAULT @d_from;\n",
    "    DECLARE d_to   DATE DEFAULT @d_to;\n",
    "\n",
    "    -- Lá»c staging theo window\n",
    "    CREATE TEMP TABLE _S AS\n",
    "    SELECT *\n",
    "    FROM `{full_stg}`\n",
    "    WHERE NgayUpdate BETWEEN d_from AND d_to;\n",
    "\n",
    "    -- MERGE cÃ³ filter partition á»Ÿ ON (T.NgayUpdate BETWEEN ...)\n",
    "    MERGE `{full_tgt}` T\n",
    "    USING (\n",
    "      SELECT tenant, `_id`, assignedTime, createTime, updateTime, name, phone,\n",
    "             user_id, user_name, user_group_id,\n",
    "             row_hash, customField_0_val, NgayUpdate, NgayAssign\n",
    "      FROM _S\n",
    "      QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY tenant, `_id`\n",
    "        ORDER BY SAFE_CAST(updateTime AS INT64) DESC\n",
    "      ) = 1\n",
    "    ) S\n",
    "    ON T.tenant = S.tenant\n",
    "       AND T._id = S._id\n",
    "       AND T.NgayUpdate BETWEEN d_from AND d_to\n",
    "    WHEN MATCHED AND (T.row_hash IS NULL OR T.row_hash != S.row_hash\n",
    "                      OR SAFE_CAST(S.updateTime AS INT64) >= SAFE_CAST(T.updateTime AS INT64)\n",
    "                      OR T.updateTime IS NULL) THEN\n",
    "      UPDATE SET\n",
    "        assignedTime = S.assignedTime,\n",
    "        createTime   = S.createTime,\n",
    "        updateTime   = S.updateTime,\n",
    "        name         = S.name,\n",
    "        phone        = S.phone,\n",
    "        user_id      = S.user_id,\n",
    "        user_name    = S.user_name,\n",
    "        user_group_id= S.user_group_id,\n",
    "        row_hash     = S.row_hash,\n",
    "        customField_0_val = S.customField_0_val,\n",
    "        NgayUpdate   = S.NgayUpdate,\n",
    "        NgayAssign   = S.NgayAssign\n",
    "    WHEN NOT MATCHED BY TARGET THEN\n",
    "      INSERT (tenant, _id, assignedTime, createTime, updateTime, name, phone, user_id,\n",
    "              user_name, user_group_id, row_hash, customField_0_val, NgayUpdate, NgayAssign)\n",
    "      VALUES (S.tenant, S._id, S.assignedTime, S.createTime, S.updateTime, S.name, S.phone, S.user_id,\n",
    "              S.user_name, S.user_group_id, S.row_hash, S.customField_0_val, S.NgayUpdate, S.NgayAssign);\n",
    "    \"\"\"\n",
    "    job_cfg = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"d_from\", \"DATE\", d_from),\n",
    "            bigquery.ScalarQueryParameter(\"d_to\",   \"DATE\", d_to),\n",
    "        ]\n",
    "    )\n",
    "    client.query(sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    # Dá»n staging theo window Ä‘á»ƒ nháº¹ báº£ng stg\n",
    "    del_sql = f\"DELETE FROM `{full_stg}` WHERE NgayUpdate BETWEEN @d_from AND @d_to\"\n",
    "    client.query(del_sql, job_config=job_cfg, location=BQ_LOCATION).result()\n",
    "\n",
    "    logger.info(f\"ðŸ§© MERGE customer done for window [{d_from}..{d_to}] & cleaned staging.\")\n",
    "\n",
    "def run_call_for_tenant(client: bigquery.Client, token_unused: str, tenant: str, email: Optional[str] = None, password: Optional[str] = None):\n",
    "    table_key = \"call_log\"\n",
    "    ck = get_ck(table_key, tenant)\n",
    "    if ck is None:\n",
    "        ck = int((datetime.now(timezone.utc) - timedelta(days=DAYS_TO_FETCH_IF_EMPTY)).timestamp()*1000)\n",
    "    # â— KhÃ´ng overlap Ä‘á»ƒ trÃ¡nh trÃ¹ng\n",
    "    cutoff = ck  # láº¥y strictly createTime > ck\n",
    "    logger.info(f\"[{tenant}][call_log] ck={ck} ({ms_to_iso(ck)}) \"\n",
    "                f\"â†’ cutoff={cutoff} ({ms_to_iso(cutoff)}); window_end=now={ms_to_iso(int(time.time()*1000))}\")\n",
    "\n",
    "    # Láº¥y email/password tá»« ACCOUNTS náº¿u khÃ´ng truyá»n\n",
    "    if (email is None) or (password is None):\n",
    "        acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "        if acc:\n",
    "            email, password = acc[\"email\"], acc[\"password\"]\n",
    "\n",
    "    try:\n",
    "        docs = fetch_desc_until(\"call\", tenant, email, password, \"createTime\", cutoff, log_prefix=f\"[{tenant}][call_log]\")\n",
    "    except requests.HTTPError as http_err:\n",
    "        if getattr(http_err.response, \"status_code\", None) == 401:\n",
    "            add_log(tenant, table_key, 0, ck, \"ERROR_401\")\n",
    "            logger.error(f\"[{tenant}][call_log] 401 after retry â€” check credentials/token scope\")\n",
    "        raise\n",
    "    if not docs:\n",
    "        add_log(tenant, table_key, 0, ck, \"NOOP\"); \n",
    "        logger.info(f\"[{tenant}][call_log] NOOP (no new docs)\"); \n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(docs)\n",
    "    if not df.empty and \"_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"_id\"], keep=\"last\")\n",
    "    keep_cols = [\"_id\",\"chargeTime\",\"createTime\",\"direction\",\"fromNumber\",\"toNumber\",\n",
    "             \"startTime\",\"endTime\",\"duration\",\"billDuration\",\"hangupCause\",\n",
    "             \"answerTime\"]\n",
    "    out = pd.DataFrame()\n",
    "    for c in keep_cols:\n",
    "        out[c] = df.get(c, pd.Series([None]*len(df)))\n",
    "    # fromUser (id + name)\n",
    "    if \"fromUser\" in df.columns:\n",
    "        out[\"fromUser__id\"] = df[\"fromUser\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "        out[\"fromUser__name\"] = df[\"fromUser\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"name\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"fromUser__id\"] = None\n",
    "        out[\"fromUser__name\"] = None\n",
    " \n",
    "    # fromGroup (only id)\n",
    "    if \"fromGroup\" in df.columns:\n",
    "        out[\"fromGroup__id\"] = df[\"fromGroup\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"fromGroup__id\"] = None\n",
    "\n",
    "    out[\"NgayTao\"] = pd.to_datetime(\n",
    "        pd.to_numeric(out[\"createTime\"], errors=\"coerce\"),\n",
    "        unit=\"ms\", utc=True\n",
    "    ).dt.date\n",
    "        \n",
    "    out[\"tenant\"] = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "\n",
    "    ensure_table_schema_call_log(client)\n",
    "    loaded = load_append(client, out, \"call_log\")\n",
    "    max_ct = int(out[\"createTime\"].max()) if \"createTime\" in out.columns and not out[\"createTime\"].empty else ck\n",
    "    if max_ct is not None and (get_ck(table_key, tenant) is None or max_ct > get_ck(table_key, tenant)):\n",
    "        set_ck(table_key, tenant, max_ct)\n",
    "    add_log(tenant, table_key, loaded, get_ck(table_key, tenant), \"APPEND\")\n",
    "    logger.info(f\"[{tenant}][call_log] APPEND loaded={loaded} new_ck={get_ck(table_key, tenant)} ({ms_to_iso(get_ck(table_key, tenant))})\")\n",
    "\n",
    "def run_staff_for_tenant(client: bigquery.Client, token_unused: str, tenant: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch full staff list for a tenant from Callio (/user) with pagination.\n",
    "    Chuáº©n hoÃ¡ vá» cÃ¡c cá»™t: _id, email, name, updateTime, createTime, group_id, tenant, row_hash\n",
    "    \"\"\"\n",
    "    # Láº¥y credential theo tenant\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    # Láº¥y token\n",
    "    tk = get_token(tenant, email, password)\n",
    "    if not tk:\n",
    "        add_log(tenant, \"staff\", 0, None, \"ERROR_LOGIN\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --------- PAGINATION ----------\n",
    "    headers = {\"token\": tk}\n",
    "    page_size = int(os.getenv(\"STAFF_PAGE_SIZE\", \"100\"))  # tuá»³ chá»‰nh qua env náº¿u cáº§n\n",
    "    page = 1\n",
    "    all_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    while True:\n",
    "        params = {\"page\": page, \"pageSize\": page_size}\n",
    "        r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "\n",
    "        # retry 401 má»™t láº§n / trang\n",
    "        if r.status_code == 401:\n",
    "            logger.warning(f\"[{tenant}][staff] 401 â†’ refreshing token and retry (page={page})\")\n",
    "            tk = get_token(tenant, email, password, force=True)\n",
    "            if not tk:\n",
    "                add_log(tenant, \"staff\", 0, None, \"ERROR_LOGIN_REFRESH\")\n",
    "                break\n",
    "            headers = {\"token\": tk}\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "\n",
    "        r.raise_for_status()\n",
    "        data = r.json() or {}\n",
    "        docs = data.get(\"docs\") or []\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "        has_next = bool(data.get(\"hasNextPage\"))\n",
    "        # fallback náº¿u API khÃ´ng tráº£ hasNextPage: dá»«ng khi trang hiá»‡n táº¡i Ã­t hÆ¡n pageSize\n",
    "        if not has_next and len(docs) < page_size:\n",
    "            break\n",
    "        if not docs:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    logger.info(f\"[{tenant}] fetched {len(all_docs)} staff\")\n",
    "\n",
    "    # --------- Chuáº©n hoÃ¡ DataFrame ----------\n",
    "    if not all_docs:\n",
    "        add_log(tenant, \"staff\", 0, None, \"NOOP\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_docs)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"_id\"]        = df.get(\"_id\", None)\n",
    "    out[\"email\"]      = df.get(\"email\", None)\n",
    "    out[\"name\"]       = df.get(\"name\", None)\n",
    "    out[\"updateTime\"] = df.get(\"updateTime\", None)\n",
    "    out[\"createTime\"] = df.get(\"createTime\", None)\n",
    "\n",
    "    # group_id tá»« object 'group'\n",
    "    if \"group\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"group\"].apply(\n",
    "            lambda x: (safe_eval(x) or {}).get(\"_id\") if isinstance(safe_eval(x), dict) else None\n",
    "        )\n",
    "    else:\n",
    "        out[\"group_id\"] = None\n",
    "\n",
    "    out[\"tenant\"]   = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "\n",
    "    # loáº¡i báº£n ghi thiáº¿u khÃ³a\n",
    "    out = out[out[\"_id\"].notna() & out[\"tenant\"].notna()].reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def run_group_for_tenant(client: bigquery.Client, token_unused: str, tenant: str):\n",
    "    acc = next((a for a in ACCOUNTS if a[\"tenant\"] == tenant), None)\n",
    "    email = acc[\"email\"] if acc else None\n",
    "    password = acc[\"password\"] if acc else None\n",
    "\n",
    "    tk = get_token(tenant, email, password)\n",
    "    docs = None\n",
    "    for ep in (\"group\",\"user-group\"):\n",
    "        try:\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/{ep}\", headers={\"token\":tk}, timeout=API_TIMEOUT)\n",
    "            if r.status_code == 401:\n",
    "                logger.warning(f\"[{tenant}][{ep}] 401 â†’ refreshing token and retry\")\n",
    "                tk = get_token(tenant, email, password, force=True)\n",
    "                r = requests.get(f\"{CALLIO_API_BASE_URL}/{ep}\", headers={\"token\":tk}, timeout=API_TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            d = r.json() or {}\n",
    "            docs = d.get(\"docs\") or d\n",
    "            if isinstance(docs, list): break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not isinstance(docs, list): docs=[]\n",
    "    df = pd.DataFrame(docs)\n",
    "    out = pd.DataFrame()\n",
    "    if \"_id\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"_id\"].astype(\"string\")\n",
    "    elif \"id\" in df.columns:\n",
    "        out[\"group_id\"] = df[\"id\"].astype(\"string\")\n",
    "    else:\n",
    "        out[\"group_id\"] = None\n",
    "    out[\"name\"] = df.get(\"name\", None)\n",
    "    out[\"tenant\"] = tenant\n",
    "    out[\"row_hash\"] = compute_row_hash(out)\n",
    "    return out\n",
    "\n",
    "# ---------- 10) ONE-SHOT JOB RUNNERS (for cron) ----------\n",
    "\n",
    "def run_customer_once(client: bigquery.Client):\n",
    "    \"\"\"\n",
    "    Cháº¡y customer cho táº¥t cáº£ tenants 1 phÃ¡t:\n",
    "    - fetch â†’ stage theo ngÃ y\n",
    "    - merge theo window min..max\n",
    "    - advance checkpoint + log\n",
    "    \"\"\"\n",
    "    now = datetime.now(timezone.utc)\n",
    "    logger.info(\"â–¶ [cron] Run customer (all tenants)\")\n",
    "    win_min, win_max = None, None\n",
    "    staged_stats: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        token = get_token(tnt, email, pwd)\n",
    "        if not token:\n",
    "            add_log(tnt, \"customer\", 0, None, \"ERROR_LOGIN\")\n",
    "            continue\n",
    "        try:\n",
    "            (d_from, d_to), rows, ck_prev, max_ut = run_customer_for_tenant(client, token, tnt)\n",
    "            if d_from and d_to:\n",
    "                win_min = d_from if (win_min is None or d_from < win_min) else win_min\n",
    "                win_max = d_to   if (win_max is None or d_to   > win_max) else win_max\n",
    "            staged_stats[tnt] = {\"rows\": rows, \"max_ut\": max_ut}\n",
    "        except Exception:\n",
    "            logger.exception(f\"[{tnt}] customer error\")\n",
    "\n",
    "    if win_min and win_max:\n",
    "        try:\n",
    "            merge_customer_window(client, win_min, win_max)\n",
    "            # MERGE OK â†’ cáº­p nháº­t CK theo tenant vÃ  ghi log MERGED\n",
    "            for tnt, st in staged_stats.items():\n",
    "                max_ut = st.get(\"max_ut\")\n",
    "                if max_ut is not None:\n",
    "                    set_ck(\"customer\", tnt, max_ut)\n",
    "                    add_log(tnt, \"customer\", st.get(\"rows\", 0), max_ut, \"MERGED\")\n",
    "                    logger.info(f\"[{tnt}][customer] MERGED window [{win_min}..{win_max}] â†’ CK = {ms_to_iso(max_ut)}\")\n",
    "        except Exception:\n",
    "            logger.exception(f\"MERGE customer window [{win_min}..{win_max}] failed (CK not advanced)\")\n",
    "\n",
    "    flush_logs(client)\n",
    "\n",
    "\n",
    "def run_call_once(client: bigquery.Client):\n",
    "    \"\"\"\n",
    "    Cháº¡y call_log cho táº¥t cáº£ tenants 1 phÃ¡t (append + advance CK).\n",
    "    \"\"\"\n",
    "    logger.info(\"â–¶ [cron] Run call_log (all tenants)\")\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        token = get_token(tnt, email, pwd)\n",
    "        if not token:\n",
    "            add_log(tnt, \"call_log\", 0, get_ck(\"call_log\", tnt), \"ERROR_LOGIN\")\n",
    "            continue\n",
    "        try:\n",
    "            run_call_for_tenant(client, token, tnt)\n",
    "        except Exception:\n",
    "            logger.exception(f\"[{tnt}] call_log error\")\n",
    "    flush_logs(client)\n",
    "\n",
    "def snapshot_staff_group(client: bigquery.Client):\n",
    "    logger.info(\"â–¶ Snapshot staff (MERGE by _id) + group (TRUNCATE)\")\n",
    "\n",
    "    staff_all, grp_all = [], []\n",
    "\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        tk = get_token(tnt, email, pwd)\n",
    "        if not tk:\n",
    "            add_log(tnt, \"staff/group\", 0, None, \"ERROR_LOGIN\")\n",
    "            logger.error(f\"[{tnt}] bá» qua staff/group do khÃ´ng login Ä‘Æ°á»£c\")\n",
    "            continue\n",
    "        try:\n",
    "            df_staff = run_staff_for_tenant(client, tk, tnt)\n",
    "            if isinstance(df_staff, pd.DataFrame) and not df_staff.empty:\n",
    "                staff_all.append(df_staff)\n",
    "\n",
    "            df_group = run_group_for_tenant(client, tk, tnt)\n",
    "            if isinstance(df_group, pd.DataFrame) and not df_group.empty:\n",
    "                grp_all.append(df_group)\n",
    "        except Exception:\n",
    "            logger.exception(f\"[{tnt}] staff/group error\")\n",
    "\n",
    "    # STAFF â†’ MERGE theo (tenant, _id) khÃ´ng staging\n",
    "    rows_staff = 0\n",
    "    if staff_all:\n",
    "        df_staff_all = pd.concat(staff_all, ignore_index=True)\n",
    "        rows_staff = merge_staff_in_memory(client, df_staff_all)\n",
    "    add_log(\"ALL\", \"staff\", rows_staff, None, \"MERGE_INMEM\")\n",
    "\n",
    "    # GROUP â†’ TRUNCATE (snapshot)\n",
    "    ensure_table_schema_group(client)\n",
    "    rows_group = 0\n",
    "    if grp_all:\n",
    "        df_grp_all = pd.concat(grp_all, ignore_index=True)\n",
    "        rows_group = load_truncate(client, df_grp_all, \"group\")\n",
    "    add_log(\"ALL\", \"group\", rows_group, None, \"SNAPSHOT_TRUNCATE\")\n",
    "\n",
    "    logger.info(f\"[staff/group] staff_merged={rows_staff}, group_truncated={rows_group}\")\n",
    "\n",
    "def run_staffgroup_once(client: bigquery.Client):\n",
    "    \"\"\"\n",
    "    Snapshot staff + group 1 phÃ¡t (staff merge, group truncate).\n",
    "    \"\"\"\n",
    "    logger.info(\"â–¶ [cron] Run staff/group snapshot\")\n",
    "    try:\n",
    "        snapshot_staff_group(client)\n",
    "    except Exception:\n",
    "        logger.exception(\"[staff/group] error\")\n",
    "    flush_logs(client)\n",
    "\n",
    "\n",
    "def run_rank_once(client: bigquery.Client):\n",
    "    \"\"\"\n",
    "    Ghi snapshot rank_mapping háº±ng ngÃ y vÃ o tuáº§n hiá»‡n táº¡i (replace partition tuáº§n).\n",
    "    \"\"\"\n",
    "    logger.info(\"â–¶ [cron] Run rank_mapping snapshot\")\n",
    "    try:\n",
    "        weekly_key = iso_week_key(datetime.now(timezone.utc))\n",
    "        rows = run_rank_mapping_weekly(client, weekly_key)\n",
    "        add_log(\"ALL\", \"rank_mapping\", rows, None, \"REPLACE_PARTITION\")\n",
    "        logger.info(f\"[rank_mapping] REPLACE_PARTITION week={weekly_key} rows={rows}\")\n",
    "    except Exception:\n",
    "        logger.exception(\"rank_mapping error\")\n",
    "    flush_logs(client)\n",
    "\n",
    "def bootstrap():\n",
    "    client = get_bq_client()\n",
    "    ensure_dataset(client)\n",
    "    ensure_update_log(client)\n",
    "    ensure_table_schema_call_log(client)\n",
    "    ensure_table_schema_staff(client)\n",
    "    ensure_table_schema_group(client)\n",
    "    ensure_table_schema_customer_main(client)\n",
    "    ensure_table_schema_customer_stg(client)\n",
    "    ensure_table_schema_rank_single(client)  # ðŸ‘ˆ thÃªm dÃ²ng nÃ y\n",
    "    warm_checkpoint_cache(client)\n",
    "    return client\n",
    "\n",
    "# ---------- 11) MAIN (cron-friendly) ----------\n",
    "def run_once(job: str = \"all\"):\n",
    "    \"\"\"\n",
    "    DÃ¹ng cho cron: cháº¡y Ä‘Ãºng 1 láº§n rá»“i thoÃ¡t.\n",
    "    job: all | customer | call | staffgroup | rank\n",
    "    \"\"\"\n",
    "    client = bootstrap()\n",
    "\n",
    "    if job == \"all\":\n",
    "        # thá»© tá»± há»£p lÃ½ Ä‘á»ƒ data liá»n máº¡ch\n",
    "        run_customer_once(client)\n",
    "        run_call_once(client)\n",
    "        run_staffgroup_once(client)\n",
    "        run_rank_once(client)\n",
    "    elif job == \"customer\":\n",
    "        run_customer_once(client)\n",
    "    elif job == \"call\":\n",
    "        run_call_once(client)\n",
    "    elif job == \"staffgroup\":\n",
    "        run_staffgroup_once(client)\n",
    "    elif job == \"rank\":\n",
    "        run_rank_once(client)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown job: {job}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", choices=[\"once\"], default=\"once\",\n",
    "                        help=\"Chá»‰ há»— trá»£ once (dÃ¹ng cron).\")\n",
    "    parser.add_argument(\"--job\", choices=[\"all\",\"customer\",\"call\",\"staffgroup\",\"rank\"],\n",
    "                        default=\"all\", help=\"Chá»n job Ä‘á»ƒ cháº¡y má»™t láº§n.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    run_once(args.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba09902-1331-41fc-ae7c-c88e1879f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hot1new] fetched 63 staff\n",
      "[hot1old] fetched 87 staff\n",
      "[hot2] fetched 63 staff\n",
      "ðŸ‘‰ Tá»•ng staff táº¥t cáº£ tenants: 213\n",
      "                        _id                    name  \\\n",
      "0  68c0d866d7682e2d4de654c6   LIÃŠN - TS10 - TÃ‚Y SÆ N   \n",
      "1  68be3f3579fc35aa82d8ca45     VY - TH13 - TUY HÃ’A   \n",
      "2  68be3f00b87ecc6ccdbb9f02  TUYá»€N - TH29 - TUY HÃ’A   \n",
      "3  68be3ed49479407729becfb0  TUYáº¾T - TH28 - TUY HÃ’A   \n",
      "4  68ae7eccb9db45c7f05e72ea                      QC   \n",
      "\n",
      "                       email   tenant  \n",
      "0   lients10tayson@gmail.com  hot1new  \n",
      "1     vyth13tuyhoa@gmail.com  hot1new  \n",
      "2  tuyenth29tuyhoa@gmail.com  hot1new  \n",
      "3  tuyetth28tuyhoa@gmail.com  hot1new  \n",
      "4        qchot1new@gmail.com  hot1new  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CALLIO_API_BASE_URL = os.getenv(\"CALLIO_API_BASE_URL\", \"https://clientapi.phonenet.io\")\n",
    "API_TIMEOUT = int(os.getenv(\"API_TIMEOUT\", \"60\"))\n",
    "\n",
    "# Thay báº±ng accounts thá»±c táº¿\n",
    "ACCOUNTS = [\n",
    "    {\"tenant\": \"hot1new\", \"email\": \"hot1new@gmail.com\", \"password\": \"Huyhoang@123\"},\n",
    "    {\"tenant\": \"hot1old\", \"email\": \"hot1old@gmail.com\", \"password\": \"Huyhoang@123\"},\n",
    "    {\"tenant\": \"hot2\",    \"email\": \"hot2@gmail.com\",    \"password\": \"Huyhoang@123\"},\n",
    "]\n",
    "\n",
    "# Cache token trong RAM\n",
    "TOKENS: Dict[str, str] = {}\n",
    "\n",
    "# ---------------- HELPER ----------------\n",
    "def get_token(tenant: str, email: str, password: str, force: bool = False) -> Optional[str]:\n",
    "    \"\"\"Login Ä‘á»ƒ láº¥y token Callio\"\"\"\n",
    "    if not force and tenant in TOKENS:\n",
    "        return TOKENS[tenant]\n",
    "    r = requests.post(\n",
    "        f\"{CALLIO_API_BASE_URL}/auth/login\",\n",
    "        json={\"email\": email, \"password\": password},\n",
    "        timeout=API_TIMEOUT,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    tk = (r.json() or {}).get(\"token\")\n",
    "    if not tk:\n",
    "        raise RuntimeError(f\"[{tenant}] Cannot get Callio token\")\n",
    "    TOKENS[tenant] = tk\n",
    "    return tk\n",
    "\n",
    "\n",
    "def fetch_all_staff_docs(tenant: str, email: str, password: str, page_size: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Láº¥y full staff theo trang\"\"\"\n",
    "    token = get_token(tenant, email, password, force=True)\n",
    "    headers = {\"token\": token}\n",
    "\n",
    "    all_docs, page = [], 1\n",
    "    while True:\n",
    "        params = {\"page\": page, \"pageSize\": page_size}\n",
    "        r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "        if r.status_code == 401:\n",
    "            token = get_token(tenant, email, password, force=True)\n",
    "            headers = {\"token\": token}\n",
    "            r = requests.get(f\"{CALLIO_API_BASE_URL}/user\", headers=headers, params=params, timeout=API_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        data = r.json() or {}\n",
    "        docs = data.get(\"docs\") or []\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "        has_next = bool(data.get(\"hasNextPage\")) or (len(docs) >= page_size)\n",
    "        if not has_next:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    total = 0\n",
    "    parts = []\n",
    "    for acc in ACCOUNTS:\n",
    "        tnt, email, pwd = acc[\"tenant\"], acc[\"email\"], acc[\"password\"]\n",
    "        docs = fetch_all_staff_docs(tnt, email, pwd, page_size=100)\n",
    "        n = len(docs)\n",
    "        total += n\n",
    "        print(f\"[{tnt}] fetched {n} staff\")\n",
    "        if n > 0:\n",
    "            df = pd.DataFrame(docs)\n",
    "            df[\"tenant\"] = tnt\n",
    "            parts.append(df)\n",
    "\n",
    "    print(f\"ðŸ‘‰ Tá»•ng staff táº¥t cáº£ tenants: {total}\")\n",
    "\n",
    "    if parts:\n",
    "        df_all = pd.concat(parts, ignore_index=True)\n",
    "        # In vÃ i cá»™t chÃ­nh\n",
    "        print(df_all[[\"_id\", \"name\", \"email\", \"tenant\"]].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
